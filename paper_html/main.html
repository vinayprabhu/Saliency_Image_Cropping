<!DOCTYPE html> 
<html> 
<head> <title>Auditing saliency cropping algorithms</title> 
<meta charset='UTF-8' /> 
<meta content='TeX4ht (http://www.cse.ohio-state.edu/~gurari/TeX4ht/)' name='generator' /> 
<link href='main.css' rel='stylesheet' type='text/css' /> 
<!--  for beautifying  --><link href='site.css' rel='stylesheet' type='text/css' /> 
<script type='text/x-mathjax-config'> MathJax.Hub.Config({ extensions: ["tex2jax.js"], jax: ["input/TeX", "output/HTML-CSS"], tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], displayMath: [ ['$$','$$'], ["\\[","\\]"] ], processEscapes: true }, "HTML-CSS": { availableFonts: ["TeX"] } }); </script> <script src='http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML' type='text/javascript'></script> 
</head><body>
   <div class='maketitle'>
                                                                     

                                                                     
                                                                     

                                                                     

<h2 class='titleHead'>Auditing saliency cropping algorithms</h2>
                           <div class='author'> <span class='cmr-12'>Abeba Birhane</span><span class='thank-mark'><a href='#tk-1'><span class='tcrm-1200'>∗</span></a></span>
<br />                   <span class='cmr-12'>University College Dublin &amp; Lero</span>
<br /><code><span class='cmtt-12'>abeba.birhane@ucdconnect.ie</span></code> <br class='and' /><span class='cmr-12'>Vinay Uday Prabhu</span><sup class='textsuperscript'><span class='cmr-10'>*</span></sup>        <span class='cmr-12'>John Whaley</span>
<br />                             <span class='cmr-12'>UnifyID Labs</span>
<br />                       <code><span class='cmsy-10x-x-120'>{</span><span class='cmtt-12'>vinay,john</span><span class='cmsy-10x-x-120'>}</span><span class='cmtt-12'>@unify.id</span></code><br /></div><br />
<div class='date'><span class='cmr-12'>December 31, 2021</span></div>
   <div class='thanks'><br /><a id='tk-1'></a><span class='thank-mark'><span class='tcrm-1095'>∗</span></span>Equal contribution</div></div>
   <section class='abstract' role='doc-abstract'> 
<h3 class='abstracttitle'>
<span class='cmbx-10'>Abstract</span>
</h3>
     <p><span class='cmr-10'>In this paper, we audit saliency cropping algorithms used by Twitter,
     </span><span class='cmr-10'>Google and Apple to investigate issues pertaining to the male-gaze
     </span><span class='cmr-10'>cropping phenomenon as well as race-gender biases that emerge in
     </span><span class='cmr-10'>post-cropping survival ratios of face-images constituting </span><!-- l. 31 --><math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'><mn>3</mn> <mo class='MathClass-bin'>×</mo> <mn>1</mn></math>
     <span class='cmr-10'>grid images. In doing so, we present the first formal empirical study
     </span><span class='cmr-10'>which suggests that the worry of a male-gaze-like image cropping phenomenon
     </span><span class='cmr-10'>on Twitter is not at all far-fetched and it </span><strong><span class='cmbx-10'>does occur</span></strong> <span class='cmr-10'>with worryingly
     </span><span class='cmr-10'>high prevalence rates in real-world full-body single-female-subject images
     </span><span class='cmr-10'>shot  with  logo-littered  backdrops.  We  uncover  that  while  all  three
     </span><span class='cmr-10'>saliency cropping frameworks considered in this paper do exhibit acute
     </span><span class='cmr-10'>racial and gender biases, Twitter’s saliency cropping framework uniquely
     </span><span class='cmr-10'>elicits high male-gaze cropping prevalence rates. In order to facilitate
     </span><span class='cmr-10'>reproducing the results presented here, we are open-sourcing both the
     </span><span class='cmr-10'>code and the datasets that we curated at </span><a class='url' href='https://vinayprabhu.github.io/Saliency_Image_Cropping/'><span class='cmtt-10'>https://vinayprabhu.github.io/Saliency_Image_Cropping/</span></a><span class='cmr-10'>. We
     </span><span class='cmr-10'>hope the computer vision community and saliency cropping researchers
     </span><span class='cmr-10'>will build on the results presented here and extend these investigations
     </span><span class='cmr-10'>to similar frameworks deployed in the real world by other companies
     </span><span class='cmr-10'>such as Microsoft and Facebook.</span>
</p> 
</section>
                                                                     

                                                                     
   <h3 class='sectionHead' id='introduction'><span class='titlemark'>1   </span> <a id='x1-10001'></a>Introduction</h3>
<p>Saliency-based Image Cropping (SIC) is currently used to algorithmically crop
user-uploaded images on most major digital technology and social media platforms,
including Twitter [<a href='#XTwittera35:online_guardian'>1</a>, <a href='#XSpeedyNe65:online_TWITTER_BLOG'>2</a>, <a href='#Xtwitter_SIC'>3</a>], Adobe [<a href='#XAdobeRes11:online'>4</a>, <a href='#XAdobe_sic'>5</a>], Google (via the <code><span class='cmtt-10x-x-109'>CROP_HINTS</span></code> API) [<a href='#XGoogle_SIC'>6</a>],
Microsoft (via the <code><span class='cmtt-10x-x-109'>generateThumbnail</span></code> and <code><span class='cmtt-10x-x-109'>areaOfInterest</span></code> APIs) [<a href='#XMicrosoft_SIC'>7</a>],
Filestack [<a href='#XSmartima8:online_Filestack'>8</a>] and Apple [<a href='#XApple_SIC_1'>9</a>, <a href='#XApple_SIC_WWDC'>10</a>]. (See the supplementary material for real-world
examples from Facebook and Google.) Although saliency-based image
cropping technology is ubiquitously integrated into major platforms, it often
operates under the radar where its existence is hidden from the people
that interact with these platforms. Recently, this technology came under
scrutiny as Twitter users shared collective frustration with the apparent
racial discriminatory tendency of the technology exemplified by the viral
<em><span class='cmti-10x-x-109'>Obama-McConnell</span></em><span class='footnote-mark'><a href='main2.html#fn1x0'><sup class='textsuperscript'>1</sup></a></span><a id='x1-1001f1'></a> 
image [<a href='#XTwittera35:online_guardian'>1</a>]. (See Figure <a href='#x1-1003r1'>1<!-- tex4ht:ref: fig:om_tga  --></a>.)
</p> 
<p>   Typically, SIC entails two phases:<span class='footnote-mark'><a href='main3.html#fn2x0'><sup class='textsuperscript'>2</sup></a></span><a id='x1-1002f2'></a> 
saliency estimation and image cropping [<a href='#Xardizzone2013saliency'>12</a>]. In the <em><span class='cmti-10x-x-109'>saliency estimation</span></em> phase,
the weights or <em><span class='cmti-10x-x-109'>“noteworthyness”</span></em> of each of the constituent pixels or regions in an
image are estimated to generate a binary mask or a continuous-valued heatmap
of pixel-wise <em><span class='cmti-10x-x-109'>“importance.”</span></em> This is then processed by an image cropping
algorithm that utilizes a segmentation policy that attempts to retain the
higher-weighted <em><span class='cmti-10x-x-109'>noteworthy</span></em> regions while discarding the regions deemed <em><span class='cmti-10x-x-109'>less
</span><span class='cmti-10x-x-109'>salient</span></em>.
</p> 
<p>   In this paper, we audit saliency cropping algorithms from three prominent
technology platforms—Twitter, Google and Apple—focusing on two areas of
inquiry. The first area pertains to the nature and extent of the prevalence of
male-gaze-like artifacts in post-cropped images emerging in real-world full-body
single-subject settings with logo-littered backdrops. The second area concerns
racial and gender-based biases observed in the post-cropping survival ratios of
<!-- l. 39 --><math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'><mn>3</mn> <mo class='MathClass-bin'>×</mo> <mn>1</mn></math>
face-image grids.
</p> 
<p>   The rest of the paper is organized as follows. Section <a href='#the-three-main-cropping-algorithms'>2<!-- tex4ht:ref: sec:frameworks  --></a> details the three
cropping frameworks considered in this paper. In Section <a href='#study-on-malegazelike-artifacts'>3<!-- tex4ht:ref: sec:mgl  --></a>, we present the
design and results of our study on male-gaze-like artifacts. Section <a href='#study-on-racial-and-gender-biases'>4<!-- tex4ht:ref: sec:cfd  --></a> covers the
details of our experiments revealing racial and gender biases using an academic
                                                                     

                                                                     
dataset. We conclude the paper in Section <a href='#conclusion-and-future-work'>5<!-- tex4ht:ref: sec:conclude  --></a>.
                                                                     

                                                                     
</p> 
<p>   </p> 
<figure class='float' id='numberline-'>
                                                                     

                                                                     
<img alt='PIC' src='figures/mgl_grids/om_tga_3.png' width='75%' /> <a id='x1-1003r1'></a>
<a id='x1-1004'></a>
<figcaption class='caption'><span class='id'>Figure 1: </span><span class='content'>(a) The Twitter SIC response to the Obama-McConnell image
for  varying  aspect  ratios.  (b)  The  Google  <code><span class='cmtt-10x-x-109'>CROP_HINTS</span></code> response  to
the  Obama-McConnell  image.  (c)  The  Apple  ABSC  response  to  the
Obama-McConnell image.                                             </span></figcaption><!-- tex4ht:label?: x1-1003r1  -->
                                                                     

                                                                     
   </figure>
   <h3 class='sectionHead' id='the-three-main-cropping-algorithms'><span class='titlemark'>2   </span> <a id='x1-20002'></a>The three main cropping algorithms</h3>
<p>In this section, we present introductory details pertaining to the three SIC
frameworks we investigate in this paper: Twitter, Google, and Apple.
   </p> 

   <h4 class='subsectionHead' id='twitters-sic'><span class='titlemark'>2.1   </span> <a id='x1-30002.1'></a>Twitter’s SIC</h4>
<p>In January 2018, Twitter announced a departure from the erstwhile
face-detection-based approach for cropping images and revealed that:
<em><span class='cmti-10x-x-109'>“A better way to crop is to focus on “salient” image regions. A region
</span><span class='cmti-10x-x-109'>having high saliency means that a person is likely to look at it when freely
</span><span class='cmti-10x-x-109'>viewing the image.”</span></em> [<a href='#XSpeedyNe65:online_TWITTER_BLOG'>2</a>]. Twitter’s SIC framework consists of two
components: The first is a saliency estimation neural network that
happens to be a knowledge-distilled Fisher-pruned version [<a href='#Xhinton2015distilling_jft'>13</a>, <a href='#Xtheis2018faster'>14</a>] of the
DeepGaze II deep learning model [<a href='#Xkummerer2016deepgaze'>15</a>]. This model produces the most
salient point (the <em><span class='cmti-10x-x-109'>focal point</span></em>) in the image that is then used by a cropping
policy<span class='footnote-mark'><a href='main4.html#fn3x0'><sup class='textsuperscript'>3</sup></a></span><a id='x1-3001f3'></a> 
that produces the final cropped image based on the desired cropping ratio (See
Section 2.3 of [<a href='#Xtwitter_SIC'>3</a>].) With this update, the claim is that Twitter’s SIC is <em><span class='cmti-10x-x-109'>“...
</span><span class='cmti-10x-x-109'>able to focus on the most interesting part of the image”</span></em> and <em><span class='cmti-10x-x-109'>“... able to
</span><span class='cmti-10x-x-109'>detect puppies, faces, text, and other objects of interest.”</span></em> We note that
while their paper [<a href='#Xtwitter_SIC'>3</a>] claims the model was trained on <em><span class='cmti-10x-x-109'>“three publicly
</span><span class='cmti-10x-x-109'>available external datasets: Borji and Itti [</span><a href='#Xborji2015cat2000'><span class='cmti-10x-x-109'>16</span></a><span class='cmti-10x-x-109'>], Jiang et al. [</span><a href='#Xjiang2015salicon'><span class='cmti-10x-x-109'>17</span></a><span class='cmti-10x-x-109'>], and
</span><span class='cmti-10x-x-109'>Judd et al. [</span><a href='#Xjudd2009learning_MIT1003'><span class='cmti-10x-x-109'>18</span></a><span class='cmti-10x-x-109'>]”</span></em>, Twitter’s blog post [<a href='#XSpeedyNe65:online_TWITTER_BLOG'>2</a>] states that <em><span class='cmti-10x-x-109'>“some third-party
</span><span class='cmti-10x-x-109'>saliency data”</span></em> was also used to train the smaller, faster Fisher-pruned
network.
   </p> 

   <h4 class='subsectionHead' id='googles-textbf-texttt-crophints'><span class='titlemark'>2.2   </span> <a id='x1-40002.2'></a>Google’s <strong><code><span class='cmtt-10x-x-109'>CROP_HINTS</span></code></strong></h4>
<p>Google offers its SIC framework under the <code><span class='cmtt-10x-x-109'>CROP_HINTS</span></code> API as part of its <em><span class='cmti-10x-x-109'>Cloud
</span><span class='cmti-10x-x-109'>Vision API</span></em> suite. While we could not find any publicly accessible dissemination
on how the underlying model is trained or on what datasets, we did
parse through the available API documentation to glean the following
information.
</p> 
<p>   As revealed in Google’s <em><span class='cmti-10x-x-109'>Features list</span></em>
                                                                     

                                                                     
documentation,<span class='footnote-mark'><a href='main5.html#fn4x0'><sup class='textsuperscript'>4</sup></a></span><a id='x1-4001f4'></a> 
the <code><span class='cmtt-10x-x-109'>CROP_HINTS</span></code> detection API ingests an image and <em><span class='cmti-10x-x-109'>“provides a bounding
</span><span class='cmti-10x-x-109'>polygon for the cropped image, a confidence score, and an importance fraction of
</span><span class='cmti-10x-x-109'>this salient region with respect to the original image for each request.”</span></em>
The confidence score is defined as the <em><span class='cmti-10x-x-109'>“confidence of this being a salient
</span><span class='cmti-10x-x-109'>region”</span></em> and is a normalized floating point values that is the range of [0,
1].<span class='footnote-mark'><a href='main6.html#fn5x0'><sup class='textsuperscript'>5</sup></a></span><a id='x1-4002f5'></a> 
</p> 
<p>   We performed all our experiments using the Python API whose documentation
page<span class='footnote-mark'><a href='main7.html#fn6x0'><sup class='textsuperscript'>6</sup></a></span><a id='x1-4003f6'></a> 
also revealed that Google’s definition of the input aspect ratio is the inverse of
that of Twitter’s.
   </p> 

   <h4 class='subsectionHead' id='apples-attentionbased-saliency-cropping'><span class='titlemark'>2.3   </span> <a id='x1-50002.3'></a>Apple’s attention-based saliency cropping</h4>
<p>Apple’s SIC framework was unveiled during the WWDC-2019 event [<a href='#XApple_SIC_WWDC'>10</a>], where
two saliency-based cropping options were made available to the developers:
Attention-Based Saliency Cropping (ABSC) and Objectness-based saliency
cropping.<span class='footnote-mark'><a href='main8.html#fn7x0'><sup class='textsuperscript'>7</sup></a></span><a id='x1-5001f7'></a> 
As stated at their event [<a href='#XApple_SIC_WWDC'>10</a>], the attention-based cropping approach was
<em><span class='cmti-10x-x-109'>human-aspected</span></em> and <em><span class='cmti-10x-x-109'>trained on eye movements</span></em> while the objectness-based
approach was trained to detect <em><span class='cmti-10x-x-109'>foreground objects</span></em> and <em><span class='cmti-10x-x-109'>trained on object
</span><span class='cmti-10x-x-109'>segmentation</span></em>. The associated slide deck [<a href='#XApple_SIC_WWDC'>10</a>] also revealed two important items of
relevance: Apple’s definition of what saliency is, and the factors that could
potentially influence the saliency of an image region. Apple defines saliency
as follows: <em><span class='cmti-10x-x-109'>“Attention-based saliency is a human-aspected saliency, and
</span><span class='cmti-10x-x-109'>by this, I mean that the attention-based saliency models were generated
</span><span class='cmti-10x-x-109'>by where people looked when they were shown a series of images. This
</span><span class='cmti-10x-x-109'>means that the heatmap reflects and highlights where people first look
</span><span class='cmti-10x-x-109'>when they’re shown an image.”</span></em> Furthermore, with regards to the factors
that influence saliency, we learn that <em><span class='cmti-10x-x-109'>“the main factors that determine
</span><span class='cmti-10x-x-109'>attention-based saliency, and what’s salient or not, are contrast, faces, subjects,
</span><span class='cmti-10x-x-109'>horizons, and light. But interestingly enough, it can also be affected by
</span><span class='cmti-10x-x-109'>perceived motion. In this example, the umbrella colors really pop, so the area
                                                                     

                                                                     
</span><span class='cmti-10x-x-109'>around the umbrella is salient, but the road is also salient because our
</span><span class='cmti-10x-x-109'>eyes try to track where the umbrella is headed.”</span></em> By parsing through the
documentation<span class='footnote-mark'><a href='main9.html#fn8x0'><sup class='textsuperscript'>8</sup></a></span><a id='x1-5002f8'></a> 
in [<a href='#XApple_SIC_1'>9</a>], we gathered that Apple’s ABSC API outputs
<!-- l. 65 --><math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'><mn>68</mn> <mo class='MathClass-bin'>×</mo> <mn>68</mn></math> shaped
“image-cell region” saliency heatmaps where each entry quantifies how salient the
pixels in the image-cell are by means of a normalized floating point saliency value
(<!-- l. 65 --><math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'> <mo class='MathClass-rel'>∈</mo> <mo class='MathClass-open'>(</mo><mn>0</mn><mo class='MathClass-punc'>,</mo><mn>1</mn><mo class='MathClass-close'>]</mo></math>),
<em><span class='cmti-10x-x-109'>“where higher values indicate higher potential for interest.”</span></em>
   </p> 

   <h4 class='subsectionHead' id='observations-and-comparisons'><span class='titlemark'>2.4   </span> <a id='x1-60002.4'></a>Observations and comparisons</h4>
<p>Firstly, we note that the notion of fixed-size input-image-independent
image-cell in Apple’s SIC framework corresponds to the salient-region notion
used by Twitter’s SIC. However, one difference is that Twitter’s salient
regions are image-dependent and identified in the saliency map using the
<code><span class='cmtt-10x-x-109'>regionprops</span></code> algorithm in the scikit-image library. (See footnote on Page 9
of [<a href='#Xtwitter_SIC'>3</a>].) Secondly, Apple’s and Google’s APIs return confidence scores along
with the model inference whereas Twitter’s SIC framework does not.
Thirdly, Google’s <code><span class='cmtt-10x-x-109'>CROP_HINTS</span></code> API is not available for free use in the
public domain, and does not return saliency values at the pixel-level,
saliency-region level, or image-cell level. Fourthly, while Google’s and
Twitter’s cropping frameworks allow for user-defined aspect ratios to be
input into the cropping policy, Apple’s ABSC framework returns a single
preset bounding box to use in cropping the input image to <em><span class='cmti-10x-x-109'>“... drop
</span><span class='cmti-10x-x-109'>uninteresting content.”</span></em> We have summarized the algorithm comparisons in
Table <a href='#x1-6001r1'>1<!-- tex4ht:ref: tab:features_tga  --></a>.
   </p> 
<div class='table'>
                                                                     

                                                                     
<p>   </p> 
<figure class='float' id='numberline-1'>
                                                                     

                                                                     
<a id='x1-6001r1'></a>
<a id='x1-6002'></a>
<figcaption class='caption'><span class='id'>Table 1: </span><span class='content'>Comparison of features across the SIC platforms.
</span></figcaption><!-- tex4ht:label?: x1-6001r2  -->
<!-- tex4ht:inline --><div class='tabular'> <table class='tabular' id='TBL-2' rules='groups'><colgroup id='TBL-2-1g'><col id='TBL-2-1' /></colgroup><colgroup id='TBL-2-2g'><col id='TBL-2-2' /></colgroup><colgroup id='TBL-2-3g'><col id='TBL-2-3' /></colgroup><colgroup id='TBL-2-4g'><col id='TBL-2-4' /></colgroup><tr class='hline'><td></td><td></td><td></td><td></td></tr><tr id='TBL-2-1-' style='vertical-align:baseline;'><td class='td11' id='TBL-2-1-1' style='text-align:left; white-space:nowrap;'>Feature <span class='cmsy-10x-x-109'>∖ </span>SIC-platform                                                                   </td><td class='td11' id='TBL-2-1-2' style='text-align:left; white-space:nowrap;'>Twitter</td><td class='td11' id='TBL-2-1-3' style='text-align:left; white-space:nowrap;'>Google</td><td class='td11' id='TBL-2-1-4' style='text-align:left; white-space:nowrap;'>Apple</td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td></tr><tr id='TBL-2-2-' style='vertical-align:baseline;'><td class='td11' id='TBL-2-2-1' style='text-align:left; white-space:nowrap;'>Custom aspect ratio                                                                        </td><td class='td11' id='TBL-2-2-2' style='text-align:left; white-space:nowrap;'>Yes     </td><td class='td11' id='TBL-2-2-3' style='text-align:left; white-space:nowrap;'>Yes     </td><td class='td11' id='TBL-2-2-4' style='text-align:left; white-space:nowrap;'>No    </td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td></tr><tr id='TBL-2-3-' style='vertical-align:baseline;'><td class='td11' id='TBL-2-3-1' style='text-align:left; white-space:nowrap;'>Returns saliency map                                                                      </td><td class='td11' id='TBL-2-3-2' style='text-align:left; white-space:nowrap;'>Yes     </td><td class='td11' id='TBL-2-3-3' style='text-align:left; white-space:nowrap;'>No     </td><td class='td11' id='TBL-2-3-4' style='text-align:left; white-space:nowrap;'>Yes   </td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td></tr><tr id='TBL-2-4-' style='vertical-align:baseline;'><td class='td11' id='TBL-2-4-1' style='text-align:left; white-space:nowrap;'>Returns model confidence                                                                 </td><td class='td11' id='TBL-2-4-2' style='text-align:left; white-space:nowrap;'>No      </td><td class='td11' id='TBL-2-4-3' style='text-align:left; white-space:nowrap;'>Yes     </td><td class='td11' id='TBL-2-4-4' style='text-align:left; white-space:nowrap;'>Yes   </td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td></tr><tr id='TBL-2-5-' style='vertical-align:baseline;'><td class='td11' id='TBL-2-5-1' style='text-align:left; white-space:nowrap;'>Available for free?                                                                           </td><td class='td11' id='TBL-2-5-2' style='text-align:left; white-space:nowrap;'>Yes     </td><td class='td11' id='TBL-2-5-3' style='text-align:left; white-space:nowrap;'>No     </td><td class='td11' id='TBL-2-5-4' style='text-align:left; white-space:nowrap;'>Yes   </td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td></tr><tr id='TBL-2-6-' style='vertical-align:baseline;'><td class='td11' id='TBL-2-6-1' style='text-align:left; white-space:nowrap;'>Python API?                                                                                 </td><td class='td11' id='TBL-2-6-2' style='text-align:left; white-space:nowrap;'>Yes     </td><td class='td11' id='TBL-2-6-3' style='text-align:left; white-space:nowrap;'>Yes     </td><td class='td11' id='TBL-2-6-4' style='text-align:left; white-space:nowrap;'>No    </td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td></tr><tr id='TBL-2-7-' style='vertical-align:baseline;'><td class='td11' id='TBL-2-7-1' style='text-align:left; white-space:nowrap;'><!-- tex4ht:inline --><div class='tabular'> <table class='tabular' id='TBL-3'><colgroup id='TBL-3-1g'><col id='TBL-3-1' /></colgroup><tr id='TBL-3-1-' style='vertical-align:baseline;'><td class='td00' id='TBL-3-1-1' style='text-align:left; white-space:nowrap;'>Documentation on training</td></tr><tr id='TBL-3-2-' style='vertical-align:baseline;'><td class='td00' id='TBL-3-2-1' style='text-align:left; white-space:nowrap;'>and cropping policy</td></tr></table>                                                           </div></td><td class='td11' id='TBL-2-7-2' style='text-align:left; white-space:nowrap;'>Yes     </td><td class='td11' id='TBL-2-7-3' style='text-align:left; white-space:nowrap;'>No     </td><td class='td11' id='TBL-2-7-4' style='text-align:left; white-space:nowrap;'>No    </td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td></tr></table>                         </div>
                                                                     

                                                                     
   </figure>
   </div>
   <h3 class='sectionHead' id='study-on-malegazelike-artifacts'><span class='titlemark'>3   </span> <a id='x1-70003'></a>Study on male-gaze-like artifacts</h3>
<p>The idea of the <em><span class='cmti-10x-x-109'>male gaze</span></em> was introduced by the British feminist film
theorist Laura Mulvey in <em><span class='cmti-10x-x-109'>“Visual Pleasure and Narrative Cinema”</span></em> [<a href='#Xmulvey1989visual'>19</a>],
authored in 1973. Mulvey situates male gaze as a process whereby women
are transformed into passive recipients of male objectification in media
representations. (See <em><span class='cmti-10x-x-109'>“Woman as image, man as bearer of the look”</span></em> in [<a href='#Xmulvey1989visual'>19</a>].)
This often manifests as a stereotypical gaze distribution characterized by
relatively longer viewing time directed at the chest and the waist-hip areas of
the women being gazed upon. The experimental research in [<a href='#Xhall2011differential_male_gaze'>20</a>], for
example, revealed that young heterosexual men display a distinctive gaze
pattern when viewing images of a twenty-year-old female subject, with
more fixations and longer viewing time dedicated to the upper body
and waist-hip region. Similarly, the authors in [<a href='#Xcornelissen2009patterns_male_gaze'>21</a>] also showed that the
so-termed <em><span class='cmti-10x-x-109'>’attractiveness fixations’</span></em> of heterosexual males did spread from the
stomach up towards the upper chest region (See Figure 1 on page 9
in [<a href='#Xcornelissen2009patterns_male_gaze'>21</a>] for the distribution heatmaps.) There was a fear on platforms
such as Twitter that the crowd-sourced data used in the training of the
saliency estimation neural network may have had a male heterosexual
labeling bias and thereby encoded a male gaze adherent fixation in the
resulting algorithm. In this section, we explore this phenomenon and
shine light on the <em><span class='cmti-10x-x-109'>whyness</span></em> of such occurrences in the saliency-cropped
images.
   </p> 

   <h4 class='subsectionHead' id='motivating-observation-and-problem-statement'><span class='titlemark'>3.1   </span> <a id='x1-80003.1'></a>Motivating observation and problem statement</h4>
   <figure class='figure' id='numberline-2'> 

                                                                     

                                                                     
                                                                     

                                                                     
<div class='center'>
<p>
</p> 
<p><img alt='PIC' src='figures/real_world_mgl.png' width='75%' /></p> 
</div>
<a id='x1-8001r2'></a>
<a id='x1-8002'></a>
<figcaption class='caption'><span class='id'>Figure 2:  </span><span class='content'>A  collage  of  real-world  user-uploaded  images  on  Twitter  that
exhibited male-gaze-like (MGL) artifacts.
</span></figcaption><!-- tex4ht:label?: x1-8001r3  -->
                                                                     

                                                                     
   </figure>
<p>   Figure <a href='#x1-8001r2'>2<!-- tex4ht:ref: fig:mgl_real  --></a> shows a collage of real-world examples of user-uploaded images on
Twitter that exhibited male-gaze-like (MGL) artifacts. Upon sifting through
the individual images, we gathered that a common theme emerged. All
these images were full-body images of people shot during red-carpet
events, such as the <em><span class='cmti-10x-x-109'>ESPYs</span></em> and the <em><span class='cmti-10x-x-109'>Emmy</span></em> awards, with a background
littered with corporate and event logos. Also, these images were <em><span class='cmti-10x-x-109'>long and
</span><span class='cmti-10x-x-109'>thin images</span></em>, i.e. images with length to width ratio being greater than 1.
At this juncture, we suspected that the saliency mechanism was also
trying to pay heed to the background logos and textual artifacts (as also
suspected in Section 3.4 of [<a href='#Xtwitter_SIC'>3</a>]) resulting in serendipitous male-gaze-like
artifacts in the cropped image. This motivated the following questions:
<br class='newline' />Q1: <em><span class='cmti-10x-x-109'>What is the underlying explanation for these male-gaze-like artifacts?</span></em>
<br class='newline' />Q2: <em><span class='cmti-10x-x-109'>Are these observations just an artifact of sampling bias?</span></em> <br class='newline' />Q3: <em><span class='cmti-10x-x-109'>Is this phenomenon unique to Twitter’s SIC model, or does it also extend to
</span><span class='cmti-10x-x-109'>Apple’s ABSC and Google’s </span><code><span class='cmitt-10x-x-109'>CROP_HINTS</span></code> <span class='cmti-10x-x-109'>frameworks as well?</span></em>
</p> 
<p>   To answer these questions, we curated a dataset of real-world images
spanning 336 images over seven different albums shot over a two year
period under varying real-world lighting conditions. We passed the images
through all three cropping frameworks presented in Section <a href='#the-three-main-cropping-algorithms'>2<!-- tex4ht:ref: sec:frameworks  --></a>. Then, we
hand-labelled the resultant cropped images into two categories: those that
exhibited MGL artifacts and those that didn’t. Finally, we computed
the MGL risk ratios for the individual albums as well as for the overall
dataset.
   </p> 

   <h4 class='subsectionHead' id='dataset-curation-and-experimental-procedure'><span class='titlemark'>3.2   </span> <a id='x1-90003.2'></a>Dataset curation and experimental procedure</h4>
<p>It was clear from the tweet-texts that the constituent images in Figure <a href='#x1-8001r2'>2<!-- tex4ht:ref: fig:mgl_real  --></a> which
inspired this experiment were from red-carpet events such as ESPY awards and
the Emmy awards ceremonies, a clue that was crucial in helping us unearth the
primal repository of such images: <em><span class='cmti-10x-x-109'>The Walt Disney Television official Flickr account
</span><span class='cmti-10x-x-109'>page</span></em>.<span class='footnote-mark'><a href='main10.html#fn9x0'><sup class='textsuperscript'>9</sup></a></span><a id='x1-9001f9'></a> 
Then, with the help of a team of human volunteers, we curated seven
sub-datasets that were event albums posted from this account that contained
images of women that also satisfied <em><strong><span class='cmbxti-10x-x-109'>all</span></strong></em> of the following criteria: <br class='newline' /><strong><span class='cmbx-10x-x-109'>Size-ratio criterion</span></strong>: The height-to-width ratio should be at least 1.25.
<br class='newline' /><strong><span class='cmbx-10x-x-109'>Full-body criterion</span></strong>: The image should contain the subject’s full body and
                                                                     

                                                                     
should not have any MGL artifacts to begin with. <br class='newline' /><strong><span class='cmbx-10x-x-109'>Consent criterion</span></strong>: The image should be clearly shot in a public setting where
it is ostensibly clear that the subject was consensually and consciously present to
be photographed as part of a public event, and bereft of any voyeuristic artifacts.
<br class='newline' /><strong><span class='cmbx-10x-x-109'>Background constraint</span></strong>: The image should contain a background littered with
corporate and event logos. <br class='newline' /><strong><span class='cmbx-10x-x-109'>Permissions criterion</span></strong>: The image should be ethically viable to be
subjected to our research plan from the point of view of frameworks
such as <em><span class='cmti-10x-x-109'>Attribution-NoDerivs 2.0 Generic license</span></em> (CC BY-ND 2.0)
that facilitates analyses with the <em><span class='cmti-10x-x-109'>attribution</span></em> and <em><span class='cmti-10x-x-109'>noDerivatives</span></em>
constraints.<span class='footnote-mark'><a href='main11.html#fn10x0'><sup class='textsuperscript'>10</sup></a></span><a id='x1-9002f10'></a> 
<br class='newline' />We curated the dataset in the form of static URL lists that we then passed as
inputs into the three above listed SIC framework APIs.
   </p> 

   <h5 class='subsubsectionHead' id='experimental-procedure'><span class='titlemark'>3.2.1   </span> <a id='x1-100003.2.1'></a>Experimental procedure</h5>
<p>As shown in Table <a href='#x1-6001r1'>1<!-- tex4ht:ref: tab:features_tga  --></a>, Google’s <code><span class='cmtt-10x-x-109'>CROP_HINTS</span></code> API does not return saliency values
but returns a bounding box as per the user-defined aspect ratio. While Apple’s
ABSC framework does return a 68 x 68 pixel buffer of floating-point saliency
values, it only provides for a preset single bounding box whose dimensions
might not be adherent to the aspect ratio being across enforced uniformly
across all frameworks. Therefore, we formulated the following strategy to
compare the results. We treat Twitter’s SIC framework to be the <em><span class='cmti-10x-x-109'>base
</span><span class='cmti-10x-x-109'>framework</span></em> and adapt the other two to perform a fair comparison. In the
case of Google’s <code><span class='cmtt-10x-x-109'>CROP_HINTS</span></code> framework, we directly use the bounding
box estimated by the model in response to the same image with the
aspect ratio set to be precisely the inverse specified for Twitter’s cropping.
Specifically, we set the aspect ratio to be 0.56 for Twitter’s SIC and
<!-- l. 125 --><math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'><mn>1</mn><mo class='MathClass-bin'>∕</mo><mn>0</mn><mo class='MathClass-punc'>.</mo><mn>56</mn> <mo class='MathClass-rel'>∼</mo> <mn>1</mn><mo class='MathClass-punc'>.</mo><mn>7857</mn></math> for
Google’s <code><span class='cmtt-10x-x-109'>CROP_HINTS</span></code> API.
</p> 
<p>   For Apple’s ABSC, we first up-sample the 68x68 saliency heatmap to fit the
image size using OpenCV’s <code><span class='cmtt-10x-x-109'>resize()</span></code> function (with the default bilinear
interpolation), and then find the <em><span class='cmti-10x-x-109'>focal point</span></em> (or the max-saliency point) from this
upsampled image. Then, we pass the co-ordinates of the focal point along with
the same universal aspect-ratio (of 0.56) into the <code><span class='cmtt-10x-x-109'>plot_crop_area()</span></code>
function<span class='footnote-mark'><a href='main12.html#fn11x0'><sup class='textsuperscript'>11</sup></a></span><a id='x1-10001f11'></a> 
                                                                     

                                                                     
to obtain the final crop. This essentially helps us produce the result that answers
the query: <em><span class='cmti-10x-x-109'>“What would the crop look like if we were to use Apple’s saliency
</span><span class='cmti-10x-x-109'>estimation model with Twitter’s cropping policy?”</span></em> that in turn helps delineate
the model bias from the vagaries of the cropping policy.
   </p> 

   <h4 class='subsectionHead' id='results-and-discussion'><span class='titlemark'>3.3   </span> <a id='x1-110003.3'></a>Results and discussion</h4>
   <figure class='figure' id='numberline-3'> 

                                                                     

                                                                     
                                                                     

                                                                     
<div class='subfigure'>
<p></p> 
<p><img alt='PIC' src='figures/mgl_grids/mgl_gr.png' width='75%' /> <a id='x1-11001r1'></a>
</p> 
<div class='caption'><span class='id'><span class='cmr-10'>(a)  </span></span><span class='content'><span class='cmr-10'>Three  examples  of  Non-MGL  crops
</span><span class='cmr-10'>with face-centric focal points.</span>          </span></div>
</div>                                                                     <div class='subfigure'>
<p></p> 
<p><img alt='PIC' src='figures/mgl_grids/mgl_br.png' width='75%' /> <a id='x1-11002r2'></a>
</p> 
<div class='caption'><span class='id'><span class='cmr-10'>(b) </span></span><span class='content'><span class='cmr-10'>Three examples of MGL crops with
</span><span class='cmr-10'>non-face-centric focal points.</span>          </span></div>
</div>                                                                     <div class='subfigure'>
<p></p> 
<p><img alt='PIC' src='figures/mgl_grids/mgl_gw.png' width='75%' /> <a id='x1-11003r3'></a>
</p> 
<div class='caption'><span class='id'><span class='cmr-10'>(c)</span>
</span><span class='content'><span class='cmr-10'>Three  examples  of  </span><em><span class='cmti-10'>fortuitous</span></em> <span class='cmr-10'>non-MGL
</span><span class='cmr-10'>crops with non-face-centric focal points.</span> </span></div>
</div> <a id='x1-11004r3'></a>
<a id='x1-11005'></a>
<div class='caption'><span class='id'>Figure 3:  </span><span class='content'>Examples  that  explain  the  <em><span class='cmti-10x-x-109'>whyness</span></em>  of  cropping  of  the  MGL
dataset images based on the locationing of the focal points.
</span></div>
                                                                     

                                                                     
   </figure>
<p>   In this subsection we present experimental results to the three questions
raised in Section <a href='#motivating-observation-and-problem-statement'>3.1<!-- tex4ht:ref: sec:motivation_mgl  --></a>.
</p> 
<p>   Q1: <em><span class='cmti-10x-x-109'>What is the underlying explanation for these male-gaze-like (MGL)
</span><span class='cmti-10x-x-109'>artifacts?</span></em>
</p> 
<p>   To answer this, we turn our attention to Figure <a href='#x1-11004r3'>3<!-- tex4ht:ref: fig:mgl_explain  --></a> that consists of example
images alongside the corresponding saliency-cell-maps output by the Twitter-SIC
framework. To summarize, we found that the saliency focal points of the images,
a key factor in deciding whether the crop suffers from MGL artifacts or not,
falls into three sub-regions in the image. In Figure <a href='#x1-11001r1'>3a<!-- tex4ht:ref: fig:mgl_gr  --></a>, we see examples
where the focal point was on the face of the image of the subject that
resulted in face-centric crops that did not suffer from MGL artifacts.
In Figure <a href='#x1-11002r2'>3b<!-- tex4ht:ref: fig:mgl_br  --></a>, we see how the focal point mapped to either the fashion
accessory worn by the celebrity (left-most image) or the event logo (the
<em><span class='cmti-10x-x-109'>ESPYs</span></em> logo in the middle image) or the corporate logos (the <em><span class='cmti-10x-x-109'>Capital
</span><span class='cmti-10x-x-109'>One</span></em> logo in the right-most image) in the background which resulted
in MGL artifacts in the final cropped image. In Figure <a href='#x1-11003r3'>3c<!-- tex4ht:ref: fig:mgl_gw  --></a>, we present
examples of cases where a <em><span class='cmti-10x-x-109'>benign</span></em> crop (free of MGL artifacts) emerged out
of <em><span class='cmti-10x-x-109'>lucky serendipity</span></em> where the focal point was not face-centric but was
actually located on a background event or corporate logo(s), but the logo
coincidentally happened to be located near the face or the top-half of the image
thereby resulting in a final crop that gives the appearance of a face-centric
crop.
</p> 
<p>   Q2: <em><span class='cmti-10x-x-109'>Are these MGL cropping observations on Twitter just an artifact of
</span><span class='cmti-10x-x-109'>sampling bias?</span></em>
</p> 
<p>   To answer this, we present Table <a href='#x1-11008r2'>2<!-- tex4ht:ref: tab:mgl_summary  --></a>, which contains the rate of
prevalence of MGL artifacts across the seven albums and 336 images.
As shown in the table, the MGL prevalence rate varied from 19% to as
high as 79%, with 138 of the total 336 images verified to suffer from
MGL artifacts. There is an overall prevalence rate of around 0.41 (95%
confidence-interval<span class='footnote-mark'><a href='main13.html#fn12x0'><sup class='textsuperscript'>12</sup></a></span><a id='x1-11006f12'></a> 
of (0.36, 0.46)) for such real-world red carpet images with logo-littered
backgrounds.
</p> 
<p>   Q3: <em><span class='cmti-10x-x-109'>Is this phenomenon unique to Twitter’s SIC model or does it
</span><span class='cmti-10x-x-109'>also extend to Apple’s ABSC and Google’s </span><code><span class='cmitt-10x-x-109'>CROP_HINTS</span></code> <span class='cmti-10x-x-109'>frameworks as
</span><span class='cmti-10x-x-109'>well?</span></em>
</p> 
<p>   Our experimental results show that both Google’s <code><span class='cmtt-10x-x-109'>CROP_HINTS</span></code> and Apple’s
ABSC frameworks did have a strong face-centric locationing of the saliency focal
                                                                     

                                                                     
point. In Figure <a href='#x1-11012r5'>5<!-- tex4ht:ref: fig:google_quasi_mgl  --></a>, we present the only three images where Google’s <code><span class='cmtt-10x-x-109'>CROP_HINTS</span></code>
bounding-box did not entirely include the entire face. Further, we found that
Apple’s ABSC consistently produced non-MGL face-centric saliency crops for
all the images, which was compelling especially given that their API
documentation<span class='footnote-mark'><a href='main14.html#fn13x0'><sup class='textsuperscript'>13</sup></a></span><a id='x1-11007f13'></a> 
educates the developer that their model does in fact pay heed to the constituent
text, signs or posters in an image.
</p> 
<p>   These findings regarding Google’s and Apple’s SIC approaches led to further
investigations of the confidence scores and the importance-ratios provided with
these APIs that allowed us to check if these were low-confidence estimates or low
importance-fraction <em><span class='cmti-10x-x-109'>lucky</span></em> estimates. In Figure <a href='#x1-11010r4'>4<!-- tex4ht:ref: fig:mgl_conf  --></a> we address this possibility by
means of album-specific scatter-plots and box-plots. As seen from Figure <a href='#x1-11010r4'>4<!-- tex4ht:ref: fig:mgl_conf  --></a>a, <a href='#x1-11010r4'>4<!-- tex4ht:ref: fig:mgl_conf  --></a>b,
and <a href='#x1-11010r4'>4<!-- tex4ht:ref: fig:mgl_conf  --></a>d, Apple’s model was slightly more confident than Google’s <code><span class='cmtt-10x-x-109'>CROP_HINTS</span></code>
model over the 336 images. Figure <a href='#x1-11010r4'>4<!-- tex4ht:ref: fig:mgl_conf  --></a>c also indicates that Google’s <code><span class='cmtt-10x-x-109'>importance
</span><span class='cmtt-10x-x-109'>fraction</span></code> associated with these images was consistently above 0.5 implying the
crop had at least 50% saliency of the entire image. Furthermore, as can
be seen in Figure <a href='#x1-11010r4'>4<!-- tex4ht:ref: fig:mgl_conf  --></a>e, neither Google’s nor Apple’s model confidence
scores yielded any clues whether Twitter’s SIC would result in MGL
crops.
</p> 
<p>   <strong><span class='cmbx-10x-x-109'>Important Note:</span></strong> At this juncture, we’d like to explicitly inspire
caution against reductionist interpretation of these results as some
sort of a validation of <em><span class='cmti-10x-x-109'>superiority</span></em> of Google’s and Apple’s saliency
cropping approaches. These are preliminary results obtained with seven
specific albums spanning 336 images and a specific aspect ratio of
<!-- l. 174 --><math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'><mn>0</mn><mo class='MathClass-punc'>.</mo><mn>56</mn></math>.
While drawing upon the aphorism of the <em><span class='cmti-10x-x-109'>”absence of evidence not being
</span><span class='cmti-10x-x-109'>evidence of absence”</span></em>, we’d like to call upon the computer vision community
as well as the ethics departments at these respective industry labs to
more rigorously test across a wider swath of datasets as well as aspect
ratios.
   </p> 
<div class='table'>
                                                                     

                                                                     
<p>   </p> 
<figure class='float' id='numberline-4'>
                                                                     

                                                                     
<a id='x1-11008r2'></a>
<a id='x1-11009'></a>
<figcaption class='caption'><span class='id'>Table 2:  </span><span class='content'>Summary  of  the  MGL  dataset  and  post  Twitter-SIC  MGL
statistics.
</span></figcaption><!-- tex4ht:label?: x1-11008r3  -->
<!-- tex4ht:inline --><div class='tabular'> <table class='tabular' id='TBL-4'><colgroup id='TBL-4-1g'><col id='TBL-4-1' /><col id='TBL-4-2' /><col id='TBL-4-3' /><col id='TBL-4-4' /><col id='TBL-4-5' /></colgroup><tr class='hline'><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-4-1-' style='vertical-align:baseline;'><td class='td11' id='TBL-4-1-1' style='text-align:center; white-space:nowrap;'>   <div class='multicolumn' style='text-align:center; white-space:nowrap;'>Album</div>    </td><td class='td11' id='TBL-4-1-2' style='text-align:center; white-space:nowrap;'>                           <div class='multicolumn' style='text-align:center; white-space:nowrap;'>Image-size</div>                              </td><td class='td11' id='TBL-4-1-3' style='text-align:center; white-space:nowrap;'><div class='multicolumn' style='text-align:center; white-space:nowrap;'><!-- l. 205 --><math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'><msub><mrow><mi>N</mi></mrow><mrow><mi mathvariant='italic'>images</mi></mrow></msub></math></div></td><td class='td11' id='TBL-4-1-4' style='text-align:center; white-space:nowrap;'><div class='multicolumn' style='text-align:center; white-space:nowrap;'><!-- l. 205 --><math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'><msub><mrow><mi>N</mi></mrow><mrow><mi mathvariant='italic'>MGL</mi></mrow></msub></math></div></td><td class='td11' id='TBL-4-1-5' style='text-align:center; white-space:nowrap;'><div class='multicolumn' style='text-align:center; white-space:nowrap;'>MGL-ratio</div>
</td></tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-4-2-' style='vertical-align:baseline;'><td class='td11' id='TBL-4-2-1' style='text-align:center; white-space:nowrap;'>   ABC-16     </td><td class='td11' id='TBL-4-2-2' style='text-align:center; white-space:nowrap;'>                          (2000, 3000)                                         </td><td class='td11' id='TBL-4-2-3' style='text-align:center; white-space:nowrap;'>                              20                                                </td><td class='td11' id='TBL-4-2-4' style='text-align:center; white-space:nowrap;'>                               5                                                 </td><td class='td11' id='TBL-4-2-5' style='text-align:center; white-space:nowrap;'>   0.25    </td>
</tr><tr id='TBL-4-3-' style='vertical-align:baseline;'><td class='td11' id='TBL-4-3-1' style='text-align:center; white-space:nowrap;'>   AMA-14     </td><td class='td11' id='TBL-4-3-2' style='text-align:center; white-space:nowrap;'>                          (1000, 1500)                                         </td><td class='td11' id='TBL-4-3-3' style='text-align:center; white-space:nowrap;'>                              43                                                </td><td class='td11' id='TBL-4-3-4' style='text-align:center; white-space:nowrap;'>                              13                                                </td><td class='td11' id='TBL-4-3-5' style='text-align:center; white-space:nowrap;'>   0.30    </td>
</tr><tr id='TBL-4-4-' style='vertical-align:baseline;'><td class='td11' id='TBL-4-4-1' style='text-align:center; white-space:nowrap;'>  EMMY-16   </td><td class='td11' id='TBL-4-4-2' style='text-align:center; white-space:nowrap;'>                          (2000, 3000)                                         </td><td class='td11' id='TBL-4-4-3' style='text-align:center; white-space:nowrap;'>                              127                                               </td><td class='td11' id='TBL-4-4-4' style='text-align:center; white-space:nowrap;'>                              24                                                </td><td class='td11' id='TBL-4-4-5' style='text-align:center; white-space:nowrap;'>   0.19    </td>
</tr><tr id='TBL-4-5-' style='vertical-align:baseline;'><td class='td11' id='TBL-4-5-1' style='text-align:center; white-space:nowrap;'>   ESPY-15    </td><td class='td11' id='TBL-4-5-2' style='text-align:center; white-space:nowrap;'>                          (2000, 3000)                                         </td><td class='td11' id='TBL-4-5-3' style='text-align:center; white-space:nowrap;'>                              45                                                </td><td class='td11' id='TBL-4-5-4' style='text-align:center; white-space:nowrap;'>                              32                                                </td><td class='td11' id='TBL-4-5-5' style='text-align:center; white-space:nowrap;'>   0.71    </td>
</tr><tr id='TBL-4-6-' style='vertical-align:baseline;'><td class='td11' id='TBL-4-6-1' style='text-align:center; white-space:nowrap;'>   ESPY-16    </td><td class='td11' id='TBL-4-6-2' style='text-align:center; white-space:nowrap;'>                          (2000, 3000)                                         </td><td class='td11' id='TBL-4-6-3' style='text-align:center; white-space:nowrap;'>                              42                                                </td><td class='td11' id='TBL-4-6-4' style='text-align:center; white-space:nowrap;'>                              33                                                </td><td class='td11' id='TBL-4-6-5' style='text-align:center; white-space:nowrap;'>   0.79    </td>
</tr><tr id='TBL-4-7-' style='vertical-align:baseline;'><td class='td11' id='TBL-4-7-1' style='text-align:center; white-space:nowrap;'>   ESPY-17    </td><td class='td11' id='TBL-4-7-2' style='text-align:center; white-space:nowrap;'>                          (2000, 3000)                                         </td><td class='td11' id='TBL-4-7-3' style='text-align:center; white-space:nowrap;'>                              37                                                </td><td class='td11' id='TBL-4-7-4' style='text-align:center; white-space:nowrap;'>                              18                                                </td><td class='td11' id='TBL-4-7-5' style='text-align:center; white-space:nowrap;'>   0.49    </td>
</tr><tr id='TBL-4-8-' style='vertical-align:baseline;'><td class='td11' id='TBL-4-8-1' style='text-align:center; white-space:nowrap;'>   TGIT-14    </td><td class='td11' id='TBL-4-8-2' style='text-align:center; white-space:nowrap;'><!-- tex4ht:inline --><div class='tabular'> <table class='tabular' id='TBL-5'><colgroup id='TBL-5-1g'><col id='TBL-5-1' /></colgroup><tr id='TBL-5-1-' style='vertical-align:baseline;'><td class='td00' id='TBL-5-1-1' style='text-align:center; white-space:nowrap;'>(1500, 2250)</td>
</tr><tr id='TBL-5-2-' style='vertical-align:baseline;'><td class='td00' id='TBL-5-2-1' style='text-align:center; white-space:nowrap;'> (2000, 3000)</td>
</tr><tr id='TBL-5-3-' style='vertical-align:baseline;'><td class='td00' id='TBL-5-3-1' style='text-align:center; white-space:nowrap;'> (1500, 2500)</td></tr></table>                                                                              </div></td><td class='td11' id='TBL-4-8-3' style='text-align:center; white-space:nowrap;'><!-- tex4ht:inline --><div class='tabular'> <table class='tabular' id='TBL-6'><colgroup id='TBL-6-1g'><col id='TBL-6-1' /></colgroup><tr id='TBL-6-1-' style='vertical-align:baseline;'><td class='td00' id='TBL-6-1-1' style='text-align:center; white-space:nowrap;'>16</td></tr><tr id='TBL-6-2-' style='vertical-align:baseline;'><td class='td00' id='TBL-6-2-1' style='text-align:center; white-space:nowrap;'> 5</td>
</tr><tr id='TBL-6-3-' style='vertical-align:baseline;'><td class='td00' id='TBL-6-3-1' style='text-align:center; white-space:nowrap;'> 1 </td></tr></table>                                                                                           </div></td><td class='td11' id='TBL-4-8-4' style='text-align:center; white-space:nowrap;'>                              13                                                </td><td class='td11' id='TBL-4-8-5' style='text-align:center; white-space:nowrap;'>   0.59    </td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-4-9-' style='vertical-align:baseline;'><td class='td11' id='TBL-4-9-1' style='text-align:center; white-space:nowrap;'><div class='multicolumn' style='text-align:center; white-space:nowrap;'>MGL-combined</div></td><td class='td11' id='TBL-4-9-2' style='text-align:center; white-space:nowrap;'>                               <div class='multicolumn' style='text-align:center; white-space:nowrap;'>-</div>                                  </td><td class='td11' id='TBL-4-9-3' style='text-align:center; white-space:nowrap;'>                              <div class='multicolumn' style='text-align:center; white-space:nowrap;'>336</div>                                 </td><td class='td11' id='TBL-4-9-4' style='text-align:center; white-space:nowrap;'>                              <div class='multicolumn' style='text-align:center; white-space:nowrap;'>138</div>                                 </td><td class='td11' id='TBL-4-9-5' style='text-align:center; white-space:nowrap;'><div class='multicolumn' style='text-align:center; white-space:nowrap;'>(0.36,0.46)</div>
</td></tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td></tr></table>                                                                                                                                                                                                                                </div>
                                                                     

                                                                     
   </figure>
   </div>
                                                                     

                                                                     
<p>   </p> 
<figure class='float' id='numberline-5'>
                                                                     

                                                                     
<div class='center'>
<p>
</p> 
<p><img alt='pict' src='figures/mgl_summary.png' width='75%'/> </p> 
</div>
<a id='x1-11010r4'></a>
<a id='x1-11011'></a>
<figcaption class='caption'><span class='id'>Figure 4: </span><span class='content'>Plots capturing the variation of the confidence scores and the
importance-fraction produced by the APIs.
</span></figcaption><!-- tex4ht:label?: x1-11010r3  -->
                                                                     

                                                                     
   </figure>
   <figure class='figure' id='numberline-6'> 

                                                                     

                                                                     
                                                                     

                                                                     
<div class='center'>
<p>
</p> 
<p><img alt='PIC' src='figures/google_quasi_mgl.png' width='50%' /></p> 
</div>
<a id='x1-11012r5'></a>
<a id='x1-11013'></a>
<figcaption class='caption'><span class='id'>Figure 5: </span><span class='content'>The three images where Google’s <code><span class='cmtt-10x-x-109'>CROP_HINTS</span></code> exuded quasi-MG
artifacts.
</span></figcaption><!-- tex4ht:label?: x1-11012r3  -->
                                                                     

                                                                     
   </figure>
   <h3 class='sectionHead' id='study-on-racial-and-gender-biases'><span class='titlemark'>4   </span> <a id='x1-120004'></a>Study on racial and gender biases</h3>
<p>
   </p> 

   <h4 class='subsectionHead' id='motivating-observation-and-problem-statement1'><span class='titlemark'>4.1   </span> <a id='x1-130004.1'></a>Motivating observation and problem statement</h4>
<p>In Figure <a href='#x1-1003r1'>1<!-- tex4ht:ref: fig:om_tga  --></a>, we present the results of the <em><span class='cmti-10x-x-109'>viral</span></em> Obama-McConnell image [<a href='#XTwittera35:online_guardian'>1</a>]
when passed through the three SIC frameworks considered here. This is a
<!-- l. 229 --><math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'><mn>3</mn> <mo class='MathClass-bin'>×</mo> <mn>1</mn></math> image grid consisting
of Barack Obama (the <!-- l. 229 --><math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'><mn>4</mn><msup><mrow><mn>4</mn></mrow><mrow><mi mathvariant='italic'>th</mi></mrow></msup></math>
president of the United States) and Mitch
McConnell<span class='footnote-mark'><a href='main15.html#fn14x0'><sup class='textsuperscript'>14</sup></a></span><a id='x1-13001f14'></a> 
separated by a rectangular patch of white pixels. An important observation that
emerges from the image is the idiosyncratic <em><span class='cmti-10x-x-109'>long</span></em> shape (width of 583 pixels and a
height of 3000 pixels) consisting of slightly elongated face profiles of the
individual images with slight shape asymmetries (Obama’s image is of size
<!-- l. 229 --><math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'><mn>583</mn> <mo class='MathClass-bin'>×</mo> <mn>838</mn></math> whereas Mitch McConnell’s
image is of size <!-- l. 229 --><math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'><mn>583</mn> <mo class='MathClass-bin'>×</mo> <mn>936</mn></math>).
In order to understand whether this <em><span class='cmti-10x-x-109'>viral</span></em> image was a one-off happenstance or indeed
a flagship example of the inherent racial bias embedded in the machine learning
models, we ran an experiment. We first created a synthetic dataset consisting of
many <!-- l. 229 --><math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'><mn>3</mn> <mo class='MathClass-bin'>×</mo> <mn>1</mn></math>
image-grids and passed these through the three SIC frameworks being considered
and computed the bias metrics. The details are presented in the sub-sections
below.
   </p> 

   <h4 class='subsectionHead' id='dataset-curation'><span class='titlemark'>4.2   </span> <a id='x1-140004.2'></a>Dataset curation</h4>
<p>In order to compare the results with Twitter’s study [<a href='#Xtwitter_SIC'>3</a>], we generated a
synthetic dataset of images sampled uniformly from the six race-gender ordered
pairs <code><span class='cmtt-10x-x-109'>[(BM,BF), (BM,WM), (BM,WF), (BF,WM), (BF,WF), (WM,WF)]</span></code>
where <code><span class='cmtt-10x-x-109'>B</span></code> is Black, <code><span class='cmtt-10x-x-109'>W</span></code> is White, <code><span class='cmtt-10x-x-109'>M</span></code> is Male, and <code><span class='cmtt-10x-x-109'>F</span></code> is Female. The constituent
<!-- l. 232 --><math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'><mn>3</mn> <mo class='MathClass-bin'>×</mo> <mn>1</mn></math> grid images were all
sized <strong><span class='cmbx-10x-x-109'>to be precisely</span></strong> <!-- l. 232 --><math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'><mn>583</mn> <mo class='MathClass-bin'>×</mo> <mn>3000</mn></math>
in order to retain the same idiosyncratic format observed in Figure <a href='#x1-1003r1'>1<!-- tex4ht:ref: fig:om_tga  --></a> with the format
                                                                     

                                                                     
being: <!-- l. 232 --><math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'><mi>I</mi> <mo class='MathClass-rel'>=</mo> <msup><mrow><mo class='MathClass-open'>[</mo><msub><mrow><mi>F</mi></mrow><mrow><mi>i</mi></mrow></msub><mo class='MathClass-punc'>,</mo><mi>W</mi><mo class='MathClass-punc'>,</mo><msub><mrow><mi>F</mi></mrow><mrow><mi>j</mi></mrow></msub><mo class='MathClass-close'>]</mo></mrow><mrow><mi>T</mi> </mrow></msup></math>. Here
<!-- l. 232 --><math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'><mi>W</mi></math> represents
the <!-- l. 232 --><math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'><mn>583</mn> <mo class='MathClass-bin'>×</mo> <mn>1226</mn></math>
sized white blank image inserted in the middle and
<!-- l. 232 --><math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'><msub><mrow><mi>F</mi></mrow><mrow><mi>i</mi></mrow></msub></math> and
<!-- l. 232 --><math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'><msub><mrow><mi>F</mi></mrow><mrow><mi>j</mi></mrow></msub></math>
represent equally-sized images of faces of individuals belonging to the
race-gender categories. Given that the two constituent images were of
heights 936 and 838 pixels in Figure <a href='#x1-1003r1'>1<!-- tex4ht:ref: fig:om_tga  --></a>, we set the height of the constituent
face images in our dataset to be the mean of the two (887 pixels) in
order to ensure that the size of the image would not emerge as a
confounding factor. Further, in order to control for other factors such as
<em><span class='cmti-10x-x-109'>saturation, size, resolution, lighting conditions, facial expressions, clothing
</span><span class='cmti-10x-x-109'>and eye gaze</span></em> that might influence saliency, we picked all the “neutral
expression”<span class='footnote-mark'><a href='main16.html#fn15x0'><sup class='textsuperscript'>15</sup></a></span><a id='x1-14001f15'></a> 
faces from the Chicago Faces Dataset (CFD) [<a href='#Xma2015chicago_cfd'>22</a>] that consists of controlled
images of volunteers that self-identified themselves as belonging to race and
genders denoted. This, would not only allow us to supplement and compare the
results from [<a href='#Xtwitter_SIC'>3</a>] but also permits us to side-step indulging in customized
mappings of race and ethnicity used in the study. (See the supplementary
section for the mappings obtained from the Jupyter Notebook shared at
<a class='url' href='https://bit.ly/3z0XuPc'><span class='cmtt-10x-x-109'>https://bit.ly/3z0XuPc</span></a>.) In Figure <a href='#x1-14002r6'>6<!-- tex4ht:ref: fig:cfd_examples  --></a>, we present one sample from each
of the six race-gender ordered pairs along with the most salient point
and the bounding box obtained when passed through Twitter’s SIC.
</p> 
<figure class='figure' id='numberline-7'> 

                                                                     

                                                                     
                                                                     

                                                                     
<div class='center'>
<p>
</p> 
<p><img alt='PIC' src='figures/cfd_examples.png' width='50%' /></p> 
</div>
<a id='x1-14002r6'></a>
<a id='x1-14003'></a>
<figcaption class='caption'><span class='id'>Figure 6: </span><span class='content'>Examples from the CFD-based synthetic dataset curated for the
study in Section <a href='#study-on-racial-and-gender-biases'>4<!-- tex4ht:ref: sec:cfd  --></a>.
</span></figcaption><!-- tex4ht:label?: x1-14002r4  -->
                                                                     

                                                                     
   </figure>
   <h4 class='subsectionHead' id='experiment-and-results'><span class='titlemark'>4.3   </span> <a id='x1-150004.3'></a>Experiment and results</h4>
<p>We generated a dataset of 3000 random images (500 each sampled from the six
race-gender configurations) and passed them through the three saliency
cropping frameworks explained in Section <a href='#study-on-malegazelike-artifacts'>3<!-- tex4ht:ref: sec:mgl  --></a> with the default aspect ratio of
<!-- l. 243 --><math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'><mn>0</mn><mo class='MathClass-punc'>.</mo><mn>56</mn></math>.
Owing to the long and thin dimensions of the images, the aspect ratio chosen and
the cropping policy, only one of the two constituent faces emerges <em><span class='cmti-10x-x-109'>unscathed</span></em> from
the cropping process allowing us to compute <em><span class='cmti-10x-x-109'>survival ratios</span></em> across the
six race-gender categories being considered. In Table <a href='#x1-15003r3'>3<!-- tex4ht:ref: tab:cfd_summary  --></a>, we present the
raw counts of which of the two categories survived the SIC across the
<!-- l. 243 --><math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'><mn>6</mn> <mo class='MathClass-bin'>×</mo> <mn>3</mn></math>
race-gender and SIC-platform combinations presented. For example, the
<code><span class='cmtt-10x-x-109'>(Twitter,BFWF)</span></code> indexed cell reads <code><span class='cmtt-10x-x-109'>WF</span></code>: 409, <code><span class='cmtt-10x-x-109'>BF</span></code>: 91, which means that when 500
<!-- l. 243 --><math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'><mn>3</mn> <mo class='MathClass-bin'>×</mo> <mn>1</mn></math>
grid-images consisting of randomly sampled Black-Female (BF) and White-Female
(WF) face images from the CFD dataset were passed through Twitter’s SIC, in
409 of those images, the White-Female face was <em><span class='cmti-10x-x-109'>preferred</span></em> over the Black-Female
face. In Figure <a href='#x1-15008r8'>8<!-- tex4ht:ref: fig:cfd_bias  --></a>, we present the results of reproducing the <em><span class='cmti-10x-x-109'>demographic parity</span></em>
analysis that computes the probabilities <em><span class='cmti-10x-x-109'>that the model favors the first subgroup
</span><span class='cmti-10x-x-109'>compared to the second</span></em> from Figure 2 in [<a href='#Xtwitter_SIC'>3</a>]. As seen, for the <code><span class='cmtt-10x-x-109'>WM-BF</span></code> and <code><span class='cmtt-10x-x-109'>WF-BM</span></code>
combinations, the erasure-rates of the faces of <strong><span class='cmbx-10x-x-109'>self-identified</span></strong> <em><span class='cmti-10x-x-109'>Black</span></em> individuals
are far higher under the conditions tested here. Additionally, as observed in
the <em><span class='cmti-10x-x-109'>Google row</span></em> of Table <a href='#x1-15003r3'>3<!-- tex4ht:ref: tab:cfd_summary  --></a>, we see the emergence of a third category
labelled <code><span class='cmtt-10x-x-109'>middle</span></code>, pertaining to images where the SIC bounding box focused
on the white space in the middle of the image. In Figure <a href='#x1-15001r7'>7<!-- tex4ht:ref: fig:google_middle  --></a>, we present
example images covering such occurrence across the six combinations
considered. We noted that the same effect exists in our initial study on
Facebook as well (See the supplementary section for a collage of examples).
In Rule 3 of Twitter’s cropping policy [<a href='#Xtwitter_SIC'>3</a>] we encounter the following
nuance that <em><span class='cmti-10x-x-109'>“If the saliency map is almost symmetric horizontally (decided
</span><span class='cmti-10x-x-109'>using a threshold on the absolute difference in value across the middle
</span><span class='cmti-10x-x-109'>vertical axis), then a center crop is performed irrespective of the aspect
</span><span class='cmti-10x-x-109'>ratio.”</span></em> We speculate that a similar rule used in Google’s internal blackbox
cropping policy might explain this behavior. (Note that this is not an
outlier occurrence and happens to 17–22% of all the images across the six
categories.) We also note that in the case of both Apple’s and Google’s
SICs, the extreme negative bias observed for White-Female faces in the
WMWF combination (“<code><span class='cmtt-10x-x-109'>WM</span></code>: 287, <code><span class='cmtt-10x-x-109'>WF</span></code>: 119, <code><span class='cmtt-10x-x-109'>middle</span></code>: 94” and “<code><span class='cmtt-10x-x-109'>WM</span></code>: 317, <code><span class='cmtt-10x-x-109'>WF</span></code>:
183” respectively) was a marked departure from Twitter’s SIC behavior
for the same images where the <code><span class='cmtt-10x-x-109'>WF</span></code> faces were preferred over <code><span class='cmtt-10x-x-109'>WM</span></code> faces.
                                                                     

                                                                     
<br class='newline' /><strong><span class='cmbx-10x-x-109'>Important note</span></strong>: As we experimented with these frameworks, it became
amply clear that we were grappling with an incredibly brittle algorithmic
pipeline replete with adversarial vulnerabilities [<a href='#Xfawzi2018adversarial_inevitable'>25</a>, <a href='#Xshafahi2018adversarial_inevitable'>26</a>]. We saw from
close quarters how trivial it was to change one aspect of the very same
base dataset (such as the height-to-width ratio or the lighting or the
background pixel value) and <em><strong><span class='cmbxti-10x-x-109'>radically transform the survival ratios</span></strong></em> across
the categories considered. Simply put, the brittleness of the cropping
frameworks made it worryingly easy to <em><span class='cmti-10x-x-109'>ethics-wash</span></em> the survival ratios in any
direction to fit a pre-concocted narrative. Hence, akin to [<a href='#Xtwitter_SIC'>3</a>], our main
contribution is in presenting a verifiable and systematic framework for
assessing the risks involved rather than the specific survival ratios that are
quintessentially a set of metrics clearly susceptible to the risks of Goodhart’s
law [<a href='#Xmanheim2018categorizing_Goodhart'>27</a>].
   </p> 
<figure class='figure' id='numberline-8'> 

                                                                     

                                                                     
                                                                     

                                                                     
<div class='center'>
<p>
</p> 
<p><img alt='PIC' src='figures/google_middle.png' width='50%' /></p> 
</div>
<a id='x1-15001r7'></a>
<a id='x1-15002'></a>
<figcaption class='caption'><span class='id'>Figure 7:  </span><span class='content'>Examples  where  neither  face  could  <em><span class='cmti-10x-x-109'>survive</span></em> the  cropping  with
Google’s <code><span class='cmtt-10x-x-109'>CROP_HINTS</span></code> framework.
</span></figcaption><!-- tex4ht:label?: x1-15001r4  -->
                                                                     

                                                                     
   </figure>
   <div class='table'>
                                                                     

                                                                     
<p>   </p> 
<figure class='float' id='numberline-9'>
                                                                     

                                                                     
<!-- tex4ht:inline --><div class='tabular'> <table class='tabular' id='TBL-7' rules='groups'><colgroup id='TBL-7-1g'><col id='TBL-7-1' /></colgroup><colgroup id='TBL-7-2g'><col id='TBL-7-2' /></colgroup><colgroup id='TBL-7-3g'><col id='TBL-7-3' /></colgroup><colgroup id='TBL-7-4g'><col id='TBL-7-4' /></colgroup><colgroup id='TBL-7-5g'><col id='TBL-7-5' /></colgroup><colgroup id='TBL-7-6g'><col id='TBL-7-6' /></colgroup><colgroup id='TBL-7-7g'><col id='TBL-7-7' /></colgroup><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-7-1-' style='vertical-align:baseline;'><td class='td11' id='TBL-7-1-1' style='text-align:center; white-space:nowrap;'><strong><span class='cmbx-10x-x-109'>SIC platform</span></strong></td><td class='td11' id='TBL-7-1-2' style='text-align:center; white-space:nowrap;'>                            <strong><span class='cmbx-10x-x-109'>BMBF</span></strong>                                      </td><td class='td11' id='TBL-7-1-3' style='text-align:center; white-space:nowrap;'>                           <strong><span class='cmbx-10x-x-109'>BMWM</span></strong>                                     </td><td class='td11' id='TBL-7-1-4' style='text-align:center; white-space:nowrap;'>                            <strong><span class='cmbx-10x-x-109'>BMWF</span></strong>                                      </td><td class='td11' id='TBL-7-1-5' style='text-align:center; white-space:nowrap;'>                            <strong><span class='cmbx-10x-x-109'>BFWM</span></strong>                                      </td><td class='td11' id='TBL-7-1-6' style='text-align:center; white-space:nowrap;'>                            <strong><span class='cmbx-10x-x-109'>BFWF</span></strong>                                      </td><td class='td11' id='TBL-7-1-7' style='text-align:center; white-space:nowrap;'>                           <strong><span class='cmbx-10x-x-109'>WMWF</span></strong>                                     </td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-7-2-' style='vertical-align:baseline;'><td class='td11' id='TBL-7-2-1' style='text-align:center; white-space:nowrap;'>   Twitter     </td><td class='td11' id='TBL-7-2-2' style='text-align:center; white-space:nowrap;'><!-- tex4ht:inline --><div class='tabular'> <table class='tabular' id='TBL-8'><colgroup id='TBL-8-1g'><col id='TBL-8-1' /></colgroup><tr id='TBL-8-1-' style='vertical-align:baseline;'><td class='td00' id='TBL-8-1-1' style='text-align:center; white-space:nowrap;'><code><span class='cmtt-10x-x-109'>BF</span></code>: 269</td></tr><tr id='TBL-8-2-' style='vertical-align:baseline;'><td class='td00' id='TBL-8-2-1' style='text-align:center; white-space:nowrap;'> <code><span class='cmtt-10x-x-109'>BM</span></code>: 231</td></tr></table>                                                                                     </div></td><td class='td11' id='TBL-7-2-3' style='text-align:center; white-space:nowrap;'><!-- tex4ht:inline --><div class='tabular'> <table class='tabular' id='TBL-9'><colgroup id='TBL-9-1g'><col id='TBL-9-1' /></colgroup><tr id='TBL-9-1-' style='vertical-align:baseline;'><td class='td00' id='TBL-9-1-1' style='text-align:center; white-space:nowrap;'><code><span class='cmtt-10x-x-109'>WM</span></code>: 294</td></tr><tr id='TBL-9-2-' style='vertical-align:baseline;'><td class='td00' id='TBL-9-2-1' style='text-align:center; white-space:nowrap;'> <code><span class='cmtt-10x-x-109'>BM</span></code>: 206</td></tr></table>                                                                                     </div></td><td class='td11' id='TBL-7-2-4' style='text-align:center; white-space:nowrap;'><!-- tex4ht:inline --><div class='tabular'> <table class='tabular' id='TBL-10'><colgroup id='TBL-10-1g'><col id='TBL-10-1' /></colgroup><tr id='TBL-10-1-' style='vertical-align:baseline;'><td class='td00' id='TBL-10-1-1' style='text-align:center; white-space:nowrap;'><code><span class='cmtt-10x-x-109'>WF</span></code>: 448</td></tr><tr id='TBL-10-2-' style='vertical-align:baseline;'><td class='td00' id='TBL-10-2-1' style='text-align:center; white-space:nowrap;'> <code><span class='cmtt-10x-x-109'>BM</span></code>: 52</td></tr></table>                                                                                     </div></td><td class='td11' id='TBL-7-2-5' style='text-align:center; white-space:nowrap;'><!-- tex4ht:inline --><div class='tabular'> <table class='tabular' id='TBL-11'><colgroup id='TBL-11-1g'><col id='TBL-11-1' /></colgroup><tr id='TBL-11-1-' style='vertical-align:baseline;'><td class='td00' id='TBL-11-1-1' style='text-align:center; white-space:nowrap;'><code><span class='cmtt-10x-x-109'>BF</span></code>: 256</td></tr><tr id='TBL-11-2-' style='vertical-align:baseline;'><td class='td00' id='TBL-11-2-1' style='text-align:center; white-space:nowrap;'> <code><span class='cmtt-10x-x-109'>WM</span></code>: 244</td></tr></table>                                                                                     </div></td><td class='td11' id='TBL-7-2-6' style='text-align:center; white-space:nowrap;'><!-- tex4ht:inline --><div class='tabular'> <table class='tabular' id='TBL-12'><colgroup id='TBL-12-1g'><col id='TBL-12-1' /></colgroup><tr id='TBL-12-1-' style='vertical-align:baseline;'><td class='td00' id='TBL-12-1-1' style='text-align:center; white-space:nowrap;'><code><span class='cmtt-10x-x-109'>WF</span></code>: 409</td></tr><tr id='TBL-12-2-' style='vertical-align:baseline;'><td class='td00' id='TBL-12-2-1' style='text-align:center; white-space:nowrap;'> <code><span class='cmtt-10x-x-109'>BF</span></code>: 91</td></tr></table>                                                                                     </div></td><td class='td11' id='TBL-7-2-7' style='text-align:center; white-space:nowrap;'><!-- tex4ht:inline --><div class='tabular'> <table class='tabular' id='TBL-13'><colgroup id='TBL-13-1g'><col id='TBL-13-1' /></colgroup><tr id='TBL-13-1-' style='vertical-align:baseline;'><td class='td00' id='TBL-13-1-1' style='text-align:center; white-space:nowrap;'><code><span class='cmtt-10x-x-109'>WF</span></code>: 351</td></tr><tr id='TBL-13-2-' style='vertical-align:baseline;'><td class='td00' id='TBL-13-2-1' style='text-align:center; white-space:nowrap;'> <code><span class='cmtt-10x-x-109'>WM</span></code>: 149</td></tr></table>                                                                                     </div></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-7-3-' style='vertical-align:baseline;'><td class='td11' id='TBL-7-3-1' style='text-align:center; white-space:nowrap;'>    Google      </td><td class='td11' id='TBL-7-3-2' style='text-align:center; white-space:nowrap;'><!-- tex4ht:inline --><div class='tabular'> <table class='tabular' id='TBL-14'><colgroup id='TBL-14-1g'><col id='TBL-14-1' /></colgroup><tr id='TBL-14-1-' style='vertical-align:baseline;'><td class='td00' id='TBL-14-1-1' style='text-align:center; white-space:nowrap;'> <code><span class='cmtt-10x-x-109'>BM</span></code>: 294  </td></tr><tr id='TBL-14-2-' style='vertical-align:baseline;'><td class='td00' id='TBL-14-2-1' style='text-align:center; white-space:nowrap;'> <code><span class='cmtt-10x-x-109'>BF</span></code>: 120</td>
</tr><tr id='TBL-14-3-' style='vertical-align:baseline;'><td class='td00' id='TBL-14-3-1' style='text-align:center; white-space:nowrap;'><code><span class='cmtt-10x-x-109'>middle</span></code>: 86</td></tr></table>                                                                                 </div></td><td class='td11' id='TBL-7-3-3' style='text-align:center; white-space:nowrap;'><!-- tex4ht:inline --><div class='tabular'> <table class='tabular' id='TBL-15'><colgroup id='TBL-15-1g'><col id='TBL-15-1' /></colgroup><tr id='TBL-15-1-' style='vertical-align:baseline;'><td class='td00' id='TBL-15-1-1' style='text-align:center; white-space:nowrap;'>  <code><span class='cmtt-10x-x-109'>BM</span></code>: 265   </td></tr><tr id='TBL-15-2-' style='vertical-align:baseline;'><td class='td00' id='TBL-15-2-1' style='text-align:center; white-space:nowrap;'> <code><span class='cmtt-10x-x-109'>WM</span></code>: 128</td>
</tr><tr id='TBL-15-3-' style='vertical-align:baseline;'><td class='td00' id='TBL-15-3-1' style='text-align:center; white-space:nowrap;'><code><span class='cmtt-10x-x-109'>middle</span></code>: 107</td></tr></table>                                                                                </div></td><td class='td11' id='TBL-7-3-4' style='text-align:center; white-space:nowrap;'><!-- tex4ht:inline --><div class='tabular'> <table class='tabular' id='TBL-16'><colgroup id='TBL-16-1g'><col id='TBL-16-1' /></colgroup><tr id='TBL-16-1-' style='vertical-align:baseline;'><td class='td00' id='TBL-16-1-1' style='text-align:center; white-space:nowrap;'>  <code><span class='cmtt-10x-x-109'>BM</span></code>: 299   </td></tr><tr id='TBL-16-2-' style='vertical-align:baseline;'><td class='td00' id='TBL-16-2-1' style='text-align:center; white-space:nowrap;'><code><span class='cmtt-10x-x-109'>middle</span></code>: 102</td>
</tr><tr id='TBL-16-3-' style='vertical-align:baseline;'><td class='td00' id='TBL-16-3-1' style='text-align:center; white-space:nowrap;'>  <code><span class='cmtt-10x-x-109'>WF</span></code>: 99    </td></tr></table>                                                                                </div></td><td class='td11' id='TBL-7-3-5' style='text-align:center; white-space:nowrap;'><!-- tex4ht:inline --><div class='tabular'> <table class='tabular' id='TBL-17'><colgroup id='TBL-17-1g'><col id='TBL-17-1' /></colgroup><tr id='TBL-17-1-' style='vertical-align:baseline;'><td class='td00' id='TBL-17-1-1' style='text-align:center; white-space:nowrap;'>  <code><span class='cmtt-10x-x-109'>BF</span></code>: 196   </td></tr><tr id='TBL-17-2-' style='vertical-align:baseline;'><td class='td00' id='TBL-17-2-1' style='text-align:center; white-space:nowrap;'> <code><span class='cmtt-10x-x-109'>WM</span></code>: 193</td>
</tr><tr id='TBL-17-3-' style='vertical-align:baseline;'><td class='td00' id='TBL-17-3-1' style='text-align:center; white-space:nowrap;'><code><span class='cmtt-10x-x-109'>middle</span></code>: 111</td></tr></table>                                                                                </div></td><td class='td11' id='TBL-7-3-6' style='text-align:center; white-space:nowrap;'><!-- tex4ht:inline --><div class='tabular'> <table class='tabular' id='TBL-18'><colgroup id='TBL-18-1g'><col id='TBL-18-1' /></colgroup><tr id='TBL-18-1-' style='vertical-align:baseline;'><td class='td00' id='TBL-18-1-1' style='text-align:center; white-space:nowrap;'>  <code><span class='cmtt-10x-x-109'>BF</span></code>: 209   </td></tr><tr id='TBL-18-2-' style='vertical-align:baseline;'><td class='td00' id='TBL-18-2-1' style='text-align:center; white-space:nowrap;'> <code><span class='cmtt-10x-x-109'>WF</span></code>: 180</td>
</tr><tr id='TBL-18-3-' style='vertical-align:baseline;'><td class='td00' id='TBL-18-3-1' style='text-align:center; white-space:nowrap;'><code><span class='cmtt-10x-x-109'>middle</span></code>: 111</td></tr></table>                                                                                </div></td><td class='td11' id='TBL-7-3-7' style='text-align:center; white-space:nowrap;'><!-- tex4ht:inline --><div class='tabular'> <table class='tabular' id='TBL-19'><colgroup id='TBL-19-1g'><col id='TBL-19-1' /></colgroup><tr id='TBL-19-1-' style='vertical-align:baseline;'><td class='td00' id='TBL-19-1-1' style='text-align:center; white-space:nowrap;'> <code><span class='cmtt-10x-x-109'>WM</span></code>: 287  </td></tr><tr id='TBL-19-2-' style='vertical-align:baseline;'><td class='td00' id='TBL-19-2-1' style='text-align:center; white-space:nowrap;'> <code><span class='cmtt-10x-x-109'>WF</span></code>: 119</td>
</tr><tr id='TBL-19-3-' style='vertical-align:baseline;'><td class='td00' id='TBL-19-3-1' style='text-align:center; white-space:nowrap;'><code><span class='cmtt-10x-x-109'>middle</span></code>: 94</td></tr></table>                                                                                 </div></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr id='TBL-7-4-' style='vertical-align:baseline;'><td class='td11' id='TBL-7-4-1' style='text-align:center; white-space:nowrap;'>    Apple       </td><td class='td11' id='TBL-7-4-2' style='text-align:center; white-space:nowrap;'><!-- tex4ht:inline --><div class='tabular'> <table class='tabular' id='TBL-20'><colgroup id='TBL-20-1g'><col id='TBL-20-1' /></colgroup><tr id='TBL-20-1-' style='vertical-align:baseline;'><td class='td00' id='TBL-20-1-1' style='text-align:center; white-space:nowrap;'><code><span class='cmtt-10x-x-109'>BF</span></code>: 339,</td></tr><tr id='TBL-20-2-' style='vertical-align:baseline;'><td class='td00' id='TBL-20-2-1' style='text-align:center; white-space:nowrap;'> <code><span class='cmtt-10x-x-109'>BM</span></code>: 161</td></tr></table>                                                                                    </div></td><td class='td11' id='TBL-7-4-3' style='text-align:center; white-space:nowrap;'><!-- tex4ht:inline --><div class='tabular'> <table class='tabular' id='TBL-21'><colgroup id='TBL-21-1g'><col id='TBL-21-1' /></colgroup><tr id='TBL-21-1-' style='vertical-align:baseline;'><td class='td00' id='TBL-21-1-1' style='text-align:center; white-space:nowrap;'><code><span class='cmtt-10x-x-109'>BM</span></code>: 363</td></tr><tr id='TBL-21-2-' style='vertical-align:baseline;'><td class='td00' id='TBL-21-2-1' style='text-align:center; white-space:nowrap;'> <code><span class='cmtt-10x-x-109'>WM</span></code>: 137</td></tr></table>                                                                                     </div></td><td class='td11' id='TBL-7-4-4' style='text-align:center; white-space:nowrap;'><!-- tex4ht:inline --><div class='tabular'> <table class='tabular' id='TBL-22'><colgroup id='TBL-22-1g'><col id='TBL-22-1' /></colgroup><tr id='TBL-22-1-' style='vertical-align:baseline;'><td class='td00' id='TBL-22-1-1' style='text-align:center; white-space:nowrap;'><code><span class='cmtt-10x-x-109'>BM</span></code>: 389</td></tr><tr id='TBL-22-2-' style='vertical-align:baseline;'><td class='td00' id='TBL-22-2-1' style='text-align:center; white-space:nowrap;'> <code><span class='cmtt-10x-x-109'>WF</span></code>: 111</td></tr></table>                                                                                     </div></td><td class='td11' id='TBL-7-4-5' style='text-align:center; white-space:nowrap;'><!-- tex4ht:inline --><div class='tabular'> <table class='tabular' id='TBL-23'><colgroup id='TBL-23-1g'><col id='TBL-23-1' /></colgroup><tr id='TBL-23-1-' style='vertical-align:baseline;'><td class='td00' id='TBL-23-1-1' style='text-align:center; white-space:nowrap;'><code><span class='cmtt-10x-x-109'>BF</span></code>: 385,</td></tr><tr id='TBL-23-2-' style='vertical-align:baseline;'><td class='td00' id='TBL-23-2-1' style='text-align:center; white-space:nowrap;'> <code><span class='cmtt-10x-x-109'>WM</span></code>: 115</td></tr></table>                                                                                    </div></td><td class='td11' id='TBL-7-4-6' style='text-align:center; white-space:nowrap;'><!-- tex4ht:inline --><div class='tabular'> <table class='tabular' id='TBL-24'><colgroup id='TBL-24-1g'><col id='TBL-24-1' /></colgroup><tr id='TBL-24-1-' style='vertical-align:baseline;'><td class='td00' id='TBL-24-1-1' style='text-align:center; white-space:nowrap;'><code><span class='cmtt-10x-x-109'>BF</span></code>: 396</td></tr><tr id='TBL-24-2-' style='vertical-align:baseline;'><td class='td00' id='TBL-24-2-1' style='text-align:center; white-space:nowrap;'> <code><span class='cmtt-10x-x-109'>WF</span></code>: 104</td></tr></table>                                                                                     </div></td><td class='td11' id='TBL-7-4-7' style='text-align:center; white-space:nowrap;'><!-- tex4ht:inline --><div class='tabular'> <table class='tabular' id='TBL-25'><colgroup id='TBL-25-1g'><col id='TBL-25-1' /></colgroup><tr id='TBL-25-1-' style='vertical-align:baseline;'><td class='td00' id='TBL-25-1-1' style='text-align:center; white-space:nowrap;'><code><span class='cmtt-10x-x-109'>WM</span></code>: 317</td></tr><tr id='TBL-25-2-' style='vertical-align:baseline;'><td class='td00' id='TBL-25-2-1' style='text-align:center; white-space:nowrap;'> <code><span class='cmtt-10x-x-109'>WF</span></code>: 183</td></tr></table>                                                                                     </div></td>
</tr><tr class='hline'><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>                                                                                                                                                                                                                                                                                                                                                                                                                                         </div>
<a id='x1-15003r3'></a>
<a id='x1-15004'></a>
<figcaption class='caption'><span class='id'>Table 3: </span><span class='content'>Face survival results of the CFD cropping experiment covering the
three SIC platforms considered.                                        </span></figcaption><!-- tex4ht:label?: x1-15003r4  -->
                                                                     

                                                                     
   </figure>
   </div>
   <figure class='figure' id='numberline-10'> 

                                                                     

                                                                     
                                                                     

                                                                     
<div class='subfigure'>
<p></p> 
<p><img alt='pict' src='figures/Twitter_cfd.png' width='50%'/>  <a id='x1-15005r1'></a>
</p> 
<div class='caption'><span class='id'><span class='cmr-10'>(a) </span></span><span class='content'><span class='cmr-10'>CFD bias results with Twitter’s SIC.</span></span></div>
</div>                                                                     <div class='subfigure'>
<p></p> 
<p><img alt='pict' src='figures/Google_cfd.png' width='50%'/>  <a id='x1-15006r2'></a>
</p> 
<div class='caption'><span class='id'><span class='cmr-10'>(b)   </span></span><span class='content'><span class='cmr-10'>CFD   bias   results   with   Google’s</span>
<code><span class='cmtt-10'>CROP_HINTS</span></code><span class='cmr-10'>.</span>                        </span></div>
</div>                                                                     <div class='subfigure'>
<p></p> 
<p><img alt='pict' src='figures/Apple_cfd.png' width='50%'/>  <a id='x1-15007r3'></a>
</p> 
<div class='caption'><span class='id'><span class='cmr-10'>(c) </span></span><span class='content'><span class='cmr-10'>CFD bias results with Apple’s ABSC.</span></span></div>
</div> <a id='x1-15008r8'></a>
<a id='x1-15009'></a>
<div class='caption'><span class='id'>Figure 8: </span><span class='content'>CFD bias results across the 3 SIC platforms.
</span></div>
                                                                     

                                                                     
   </figure>
   <h3 class='sectionHead' id='conclusion-and-future-work'><span class='titlemark'>5   </span> <a id='x1-160005'></a>Conclusion and future work</h3>
<p>The recent controversy [<a href='#XTwittera35:online_guardian'>1</a>] surrounding racial and gender bias in Twitter’s
saliency cropping framework lead to a self-directed non-peer-reviewed audit by
Twitter recently published on ArXiv [<a href='#Xtwitter_SIC'>3</a>]. However, saliency cropping frameworks
are not Twitter’s problem alone and are ubiquitously deployed as part of
computer vision API suites by many other technology behemoths such as
Google [<a href='#XGoogle_SIC'>6</a>], Apple [<a href='#XApple_SIC_1'>9</a>, <a href='#XApple_SIC_WWDC'>10</a>], Microsoft [<a href='#XMicrosoft_SIC'>7</a>] and Facebook among others.
In this paper, we publish an audit comparing the SIC frameworks of
Twitter, Google and Apple. In doing so, we address two broad issues
surrounding race-gender bias and the male-gaze artifacts found in post-cropped
images. To this end, the race-gender bias study is complementary to [<a href='#Xtwitter_SIC'>3</a>],
albeit carried out with a different academic dataset (Chicago Faces [<a href='#Xma2015chicago_cfd'>22</a>])
that controls for confounding factors such as <em><span class='cmti-10x-x-109'>saturation, size, resolution,
</span><span class='cmti-10x-x-109'>lighting conditions, facial expressions, clothing and eye gaze</span></em>. All the
experiments presented in this paper are systematic empirical evaluations
involving images whose formatting and sourcing mirrors the exemplar images
observed on the real-world Twitter-timeline. The dimensions of the synthetic
<!-- l. 297 --><math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'><mn>3</mn> <mo class='MathClass-bin'>×</mo> <mn>1</mn></math>
image grids were set to replicate precisely the (in)famous Obama-McConnell
image and the sourcing of the male-gaze analysis dataset(s) was directly inspired
by the specific images that Twitter users uploaded of celebrities during
red-carpet events such as the <em><span class='cmti-10x-x-109'>ESPYs</span></em> and the <em><span class='cmti-10x-x-109'>Emmys</span></em>.
</p> 
<p>   Our investigations revealed that much akin to Twitter’s SIC framework, Google’s
and Apple’s SIC frameworks also exhibit idiosyncratic face-erasure phenomena and
acute racial and gender biases. Further, we also discovered that under realistic
real-world conditions involving long-and-thin-dimensioned full-body images of
women with corporate logo-littered backgrounds, the risk of male-gaze-like crops
with Twitter’s SIC framework can be significantly high (138 out of 336 images, or
<!-- l. 299 --><math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'> <mo class='MathClass-rel'>∼</mo> <mn>41</mn><mi>%</mi></math>).
</p> 
<p>   Through this study, we hope to not only inform and inspire further audits of
other saliency cropping frameworks belonging to Facebook and Microsoft with
varying aspect ratios and larger datasets, but also urge Google and Apple to take
a cue from Twitter’s admirable efforts and disseminate more detailed
documentation pertaining how their models were trained and what datasets were
used.
</p> 
<p>
   </p> 

   <h3 class='likesectionHead' id='references'><a id='x1-170005'></a>References</h3>
                                                                     

                                                                     
<p>
    </p> 
<div class='thebibliography'>
    <p class='bibitem'><span class='biblabel'>
 <a id='XTwittera35:online_guardian'></a>[1] <span class='bibsp'>   </span></span>Alex      Hern.                Twitter      apologises      for      ’racist’
    image-cropping    algorithm    —    Twitter    —    The    Guardian.
    <a class='url' href='https://www.theguardian.com/technology/2020/sep/21/twitter-apologises-for-racist-image-cropping-algorithm'><span class='cmtt-10x-x-109'>https://www.theguardian.com/technology/2020/sep/21/twitter-apologises-for-racist-image-cropping-algorithm</span></a>,
    Sep 2020. (Accessed on 10/20/2020).
    </p>
    <p class='bibitem'><span class='biblabel'>
 <a id='XSpeedyNe65:online_TWITTER_BLOG'></a>[2] <span class='bibsp'>   </span></span>Zehan Wang        Lucas Theis.                     Speedy        neural
    networks        for        smart        auto-cropping        of        images.
    <a class='url' href='https://blog.twitter.com/engineering/en_us/topics/infrastructure/2018/Smart-Auto-Cropping-of-Images.html'><span class='cmtt-10x-x-109'>https://blog.twitter.com/engineering/en_us/topics/infrastructure/2018/Smart-Auto-Cropping-of-Images.html</span></a>,
    January 2018. (Accessed on 10/19/2020).
    </p>
    <p class='bibitem'><span class='biblabel'>
 <a id='Xtwitter_SIC'></a>[3] <span class='bibsp'>   </span></span>Kyra  Yee,  Uthaipon  Tantipongpipat,  and  Shubhanshu  Mishra.
    Image  cropping  on  Twitter:  Fairness  metrics,  their  limitations,  and
    the importance of representation, design, and agency.  <span class='cmti-10x-x-109'>arXiv preprint
    </span><span class='cmti-10x-x-109'>arXiv:2105.08667</span>, 2021.
    </p>
    <p class='bibitem'><span class='biblabel'>
 <a id='XAdobeRes11:online'></a>[4] <span class='bibsp'>   </span></span>Adobe. Adobe research <!-- l. 28 --><math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'><mo class='MathClass-rel'>≫</mo></math> search
    results <!-- l. 28 --><math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'><mo class='MathClass-rel'>≫</mo></math> cropping.
    <a class='url' href='https://research.adobe.com/?s=cropping&amp;researcharea=&amp;contenttype=&amp;searchsort='><span class='cmtt-10x-x-109'>https://research.adobe.com/?s=cropping&amp;researcharea=&amp;contenttype=&amp;searchsort=</span></a>,
    December 2020. (Accessed on 12/05/2020).
    </p>
    <p class='bibitem'><span class='biblabel'>
 <a id='XAdobe_sic'></a>[5] <span class='bibsp'>   </span></span>Lauren   Friedman.       ICYMI:   Adobe   summit   sneaks   2019.
    <a class='url' href='https://blog.adobe.com/en/2019/03/28/icymi-adobe-summit-sneaks-2019.html\#gs.9a62ez'><span class='cmtt-10x-x-109'>https://blog.adobe.com/en/2019/03/28/icymi-adobe-summit-sneaks-2019.html\#gs.9a62ez</span></a>,
    March 2019. (Accessed on 08/18/2021).
    </p>
    <p class='bibitem'><span class='biblabel'>
 <a id='XGoogle_SIC'></a>[6] <span class='bibsp'>   </span></span>Google Cloud documentation. Detect crop hints  —  Cloud Vision
    api                                                                                       .
    <a class='url' href='https://cloud.google.com/vision/docs/detecting-crop-hints'><span class='cmtt-10x-x-109'>https://cloud.google.com/vision/docs/detecting-crop-hints</span></a>,
    Apr 2019. (Accessed on 08/18/2021).
    </p>
    <p class='bibitem'><span class='biblabel'>
 <a id='XMicrosoft_SIC'></a>[7] <span class='bibsp'>   </span></span>Patrick Farley            et al.                                   Generating
                                                                     

                                                                     
    smart-cropped        thumbnails        with        computer        vision.
    <a class='url' href='https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/concept-generating-thumbnails'><span class='cmtt-10x-x-109'>https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/concept-generating-thumbnails</span></a>,
    May 2020. (Accessed on 08/18/2021).
    </p>
    <p class='bibitem'><span class='biblabel'>
 <a id='XSmartima8:online_Filestack'></a>[8] <span class='bibsp'>   </span></span>Tomek         Roszczynialski.                         Smart         image
    cropping         using         saliency         <span class='tcrm-1095'>•         </span>Filestack         blog.
    <a class='url' href='https://blog.filestack.com/thoughts-and-knowledge/smart-image-cropping-using-saliency/'><span class='cmtt-10x-x-109'>https://blog.filestack.com/thoughts-and-knowledge/smart-image-cropping-using-saliency/</span></a>,
    August 2020. (Accessed on 10/19/2020).
    </p>
    <p class='bibitem'><span class='biblabel'>
 <a id='XApple_SIC_1'></a>[9] <span class='bibsp'>   </span></span>Apple Developer Documentation. Cropping images using saliency.
    <a class='url' href='https://developer.apple.com/documentation/vision/cropping_images_using_saliency'><span class='cmtt-10x-x-109'>https://developer.apple.com/documentation/vision/cropping_images_using_saliency</span></a>,
    June 2019. (Accessed on 08/17/2021).
    </p>
    <p class='bibitem'><span class='biblabel'>
<a id='XApple_SIC_WWDC'></a>[10] <span class='bibsp'>   </span></span>Brittany Weinert et al. Understanding images in vision framework
    — WWDC19.  <a class='url' href='https://apple.co/37VsIeE'><span class='cmtt-10x-x-109'>https://apple.co/37VsIeE</span></a>, June 2019.  (Accessed on
    08/17/2021).
    </p>
    <p class='bibitem'><span class='biblabel'>
<a id='Xlu2019endtoend'></a>[11] <span class='bibsp'>   </span></span>Peng Lu, Hao Zhang, Xujun Peng, and Xiaofu Jin. An end-to-end
    neural  network  for  image  cropping  by  learning  composition  from
    aesthetic photos. <span class='cmti-10x-x-109'>arXiv preprint arXiv:1907.01432</span>, 2019.
    </p>
    <p class='bibitem'><span class='biblabel'>
<a id='Xardizzone2013saliency'></a>[12] <span class='bibsp'>   </span></span>Edoardo  Ardizzone,  Alessandro  Bruno,  and  Giuseppe  Mazzola.
    Saliency based image cropping. In <span class='cmti-10x-x-109'>International Conference on Image
    </span><span class='cmti-10x-x-109'>Analysis and Processing</span>, pages 773–782. Springer, 2013.
    </p>
    <p class='bibitem'><span class='biblabel'>
<a id='Xhinton2015distilling_jft'></a>[13] <span class='bibsp'>   </span></span>Geoffrey  Hinton,  Oriol  Vinyals,  and  Jeff  Dean.    Distilling  the
    knowledge in a neural network. <span class='cmti-10x-x-109'>arXiv preprint arXiv:1503.02531</span>, 2015.
    </p>
    <p class='bibitem'><span class='biblabel'>
<a id='Xtheis2018faster'></a>[14] <span class='bibsp'>   </span></span>Lucas  Theis,  Iryna  Korshunova,  Alykhan  Tejani,  and  Ferenc
    Huszár. Faster gaze prediction with dense networks and fisher pruning.
    <span class='cmti-10x-x-109'>arXiv preprint arXiv:1801.05787</span>, 2018.
                                                                     

                                                                     
    </p>
    <p class='bibitem'><span class='biblabel'>
<a id='Xkummerer2016deepgaze'></a>[15] <span class='bibsp'>   </span></span>Matthias Kümmerer, Thomas SA Wallis, and Matthias Bethge.
    DeepGaze II: Reading fixations from deep features trained on object
    recognition. <span class='cmti-10x-x-109'>arXiv preprint arXiv:1610.01563</span>, 2016.
    </p>
    <p class='bibitem'><span class='biblabel'>
<a id='Xborji2015cat2000'></a>[16] <span class='bibsp'>   </span></span>Ali Borji and Laurent Itti.  Cat2000: A large scale fixation dataset
    for boosting saliency research. <span class='cmti-10x-x-109'>arXiv preprint arXiv:1505.03581</span>, 2015.
    </p>
    <p class='bibitem'><span class='biblabel'>
<a id='Xjiang2015salicon'></a>[17] <span class='bibsp'>   </span></span>Ming  Jiang,  Shengsheng  Huang,  Juanyong  Duan,  and  Qi Zhao.
    Salicon: Saliency in context. In <span class='cmti-10x-x-109'>Proceedings of the IEEE conference on
    </span><span class='cmti-10x-x-109'>computer vision and pattern recognition</span>, pages 1072–1080, 2015.
    </p>
    <p class='bibitem'><span class='biblabel'>
<a id='Xjudd2009learning_MIT1003'></a>[18] <span class='bibsp'>   </span></span>Tilke Judd, Krista Ehinger, Frédo Durand, and Antonio Torralba.
    Learning  to  predict  where  humans  look.     In  <span class='cmti-10x-x-109'>2009  IEEE  12th
    </span><span class='cmti-10x-x-109'>international conference on computer vision</span>, pages 2106–2113. IEEE,
    2009.
    </p>
    <p class='bibitem'><span class='biblabel'>
<a id='Xmulvey1989visual'></a>[19] <span class='bibsp'>   </span></span>Laura Mulvey. Visual pleasure and narrative cinema. In <span class='cmti-10x-x-109'>Visual and
    </span><span class='cmti-10x-x-109'>other pleasures</span>, pages 14–26. Springer, 1989.
    </p>
    <p class='bibitem'><span class='biblabel'>
<a id='Xhall2011differential_male_gaze'></a>[20] <span class='bibsp'>   </span></span>Charlotte  Hall,  Todd  Hogue,  and  Kun  Guo.    Differential  gaze
    behavior towards sexually preferred and non-preferred human figures.
    <span class='cmti-10x-x-109'>Journal of Sex Research</span>, 48(5):461–469, 2011.
    </p>
    <p class='bibitem'><span class='biblabel'>
<a id='Xcornelissen2009patterns_male_gaze'></a>[21] <span class='bibsp'>   </span></span>Piers L Cornelissen, Peter JB Hancock, Vesa Kiviniemi, Hannah R
    George,  and  Martin J  Tovée.    Patterns  of  eye  movements  when
    male and female observers judge female attractiveness, body fat and
    waist-to-hip  ratio.   <span class='cmti-10x-x-109'>Evolution  and  Human  Behavior</span>,  30(6):417–428,
    2009.
    </p>
    <p class='bibitem'><span class='biblabel'>
<a id='Xma2015chicago_cfd'></a>[22] <span class='bibsp'>   </span></span>Debbie S Ma, Joshua Correll, and Bernd Wittenbrink. The Chicago
                                                                     

                                                                     
    face database: A free stimulus set of faces and norming data. <span class='cmti-10x-x-109'>Behavior
    </span><span class='cmti-10x-x-109'>research methods</span>, 47(4):1122–1135, 2015.
    </p>
    <p class='bibitem'><span class='biblabel'>
<a id='Xaclu_emotion'></a>[23] <span class='bibsp'>   </span></span>Jay   Stanley.        Experts   say   ’emotion   recognition’   lacks
    scientific     foundation     —     american     civil     liberties     union.
    <a class='url' href='https://www.aclu.org/blog/privacy-technology/surveillance-technologies/experts-say-emotion-recognition-lacks-scientific'><span class='cmtt-10x-x-109'>https://www.aclu.org/blog/privacy-technology/surveillance-technologies/experts-say-emotion-recognition-lacks-scientific</span></a>,
    July 2019. (Accessed on 10/17/2021).
    </p>
    <p class='bibitem'><span class='biblabel'>
<a id='Xbarrett2019emotional'></a>[24] <span class='bibsp'>   </span></span>Lisa Feldman  Barrett,  Ralph  Adolphs,  Stacy  Marsella,  Aleix M
    Martinez,  and  Seth D  Pollak.   Emotional  expressions  reconsidered:
    Challenges  to  inferring  emotion  from  human  facial  movements.
    <span class='cmti-10x-x-109'>Psychological science in the public interest</span>, 20(1):1–68, 2019.
    </p>
    <p class='bibitem'><span class='biblabel'>
<a id='Xfawzi2018adversarial_inevitable'></a>[25] <span class='bibsp'>   </span></span>Alhussein  Fawzi,  Hamza  Fawzi,  and  Omar  Fawzi.    Adversarial
    vulnerability  for  any  classifier.   In  <span class='cmti-10x-x-109'>Advances  in  neural  information
    </span><span class='cmti-10x-x-109'>processing systems</span>, pages 1178–1187, 2018.
    </p>
    <p class='bibitem'><span class='biblabel'>
<a id='Xshafahi2018adversarial_inevitable'></a>[26] <span class='bibsp'>   </span></span>Ali Shafahi, W Ronny Huang, Christoph Studer, Soheil Feizi, and
    Tom Goldstein.  Are adversarial examples inevitable?  <span class='cmti-10x-x-109'>arXiv preprint
    </span><span class='cmti-10x-x-109'>arXiv:1809.02104</span>, 2018.
    </p>
    <p class='bibitem'><span class='biblabel'>
<a id='Xmanheim2018categorizing_Goodhart'></a>[27] <span class='bibsp'>   </span></span>David Manheim and Scott Garrabrant.   Categorizing variants of
    goodhart’s law. <span class='cmti-10x-x-109'>arXiv preprint arXiv:1803.04585</span>, 2018.
</p>
    </div>
    
</body> 
</html>
