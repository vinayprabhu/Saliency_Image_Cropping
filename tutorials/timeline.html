
<p style="color: red; font-weight: bold">>>>>>  gd2md-html alert:  ERRORs: 0; WARNINGs: 0; ALERTS: 6.</p>
<ul style="color: red; font-weight: bold"><li>See top comment block for details on ERRORs and WARNINGs. <li>In the converted Markdown or HTML, search for inline alerts that start with >>>>>  gd2md-html alert:  for specific instances that need correction.</ul>

<p style="color: red; font-weight: bold">Links to alert messages:</p><a href="#gdcalert1">alert1</a>
<a href="#gdcalert2">alert2</a>
<a href="#gdcalert3">alert3</a>
<a href="#gdcalert4">alert4</a>
<a href="#gdcalert5">alert5</a>
<a href="#gdcalert6">alert6</a>

<p style="color: red; font-weight: bold">>>>>> PLEASE check and correct alert issues and delete this message and the inline alerts.<hr></p>

<p>
<p style="text-align: right">
üè°<a href="https://vinayprabhu.github.io/Saliency_Image_Cropping/">Home</a></p>
</p>
<p>
<span style="text-decoration:underline;">üïêTIMELINE OF THE PROJECTüïê</span>
</p>
<p>
<strong>TL-DR:<span style="text-decoration:underline;">ü¶ñ</span></strong>
</p>
<p>
</p>
<table>
  <tr>
   <td>Date
   </td>
   <td>Event
   </td>
  </tr>
  <tr>
   <td>September 19, 2020
   </td>
   <td>Discovery on twitter and first simple set of experiments üò¶
   </td>
  </tr>
  <tr>
   <td>September 21, 2020
   </td>
   <td> ‚úçFirst <a
href="https://vinayprabhu.medium.com/on-the-twitter-cropping-controversy-critique-clarifications-and-comments-7ac66154f687">blogpost</a>
titled ‚ÄòOn the twitter cropping controversy: Critique, clarifications &
comments‚Äô
   </td>
  </tr>
  <tr>
   <td>October 2, 2020
   </td>
   <td>‚úç Second blogpost on <em><a
href="https://vinayprabhu.medium.com/scrutinizing-saliency-based-image-cropping-6b7a70cfb4f1">Scrutinizing
Saliency Based Image Cropping</a></em>
   </td>
  </tr>
  <tr>
   <td>October 9, 2020
   </td>
   <td>A 30 min discussion with 2 members of a ‚Äòmeta-team‚Äô within Twitter
   </td>
  </tr>
  <tr>
   <td>April 2, 2021
   </td>
   <td>üòç ‚úç First workshop paper submission to the <a
href="https://sites.google.com/view/beyond-fairness-cv/accepted-papers?authuser=0">BeyondFairCV
workshop</a> (as Submission 21)
   </td>
  </tr>
  <tr>
   <td>May 23, 2021
   </td>
   <td>üòé Acceptance notification (Original decision  date was Apr 23, but got
postponed on account of the pandemic)
   </td>
  </tr>
  <tr>
   <td>June 21, 2021
   </td>
   <td>‚úç Submission of the camera-ready version of the paper to the organizers
of the workshop
   </td>
  </tr>
  <tr>
   <td>June 25, 2021
   </td>
   <td>üôå Paper presentation at the <a
href="https://sites.google.com/view/beyond-fairness-cv/home">workshop</a> via
Discord
   </td>
  </tr>
  <tr>
   <td>Aug 11, 2021
   </td>
   <td>‚úç Second paper submitted @ <a
href="https://wacv2022.thecvf.com/home">WACV-2022</a> (Round-2)
   </td>
  </tr>
  <tr>
   <td> Oct 4, 2021
   </td>
   <td>Acceptance notification at WACV-2022  üôå
   </td>
  </tr>
  <tr>
   <td>January 06, 2022
   </td>
   <td>üòé üéä Paper  + Poster presentation at WACV-2022
   </td>
  </tr>
</table>
<p>
So, we began working on this project on September 19, 2020, when this happened!
</p>
<p>
<p id="gdcalert1" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html
alert: inline image link here (to images_timeline/image1.png). Store image on your image
server and adjust path/filename/extension if necessary. </span><br>(<a
href="#">Back to top</a>)(<a href="#gdcalert2">Next alert</a>)<br><span
style="color: red; font-weight: bold">>>>>> </span></p>
<img src="images_timeline/image1.png" width="" alt="alt_text" title="image_tooltip">
</p>
<p>
For the next couple of hours, we began experimenting on how ridiculously biased
and brittle the deployed algorithm was!
</p>
<p>
<p id="gdcalert2" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html
alert: inline image link here (to images_timeline/image2.png). Store image on your image
server and adjust path/filename/extension if necessary. </span><br>(<a
href="#">Back to top</a>)(<a href="#gdcalert3">Next alert</a>)<br><span
style="color: red; font-weight: bold">>>>>> </span></p>
<img src="images_timeline/image2.png" width="" alt="alt_text" title="image_tooltip">
</p>
<p>
A couple of days later (Sep 21, 2020), we published the first draft of our
thoughts as a <a
href="https://vinayprabhu.medium.com/on-the-twitter-cropping-controversy-critique-clarifications-and-comments-7ac66154f687">blog
post</a> and then on October 2, 2020, our second <a
href="https://vinayprabhu.medium.com/scrutinizing-saliency-based-image-cropping-6b7a70cfb4f1">blog
post</a> unveiled our collaboration with the amazing folks at Gradio (Ali, Abid
and Dawood) that resulted in the interactive <a
href="https://www.gradio.app/hub/aliabid94/saliency">GradioApp</a> where users
could upload images of their choice and see how the SoTA saliency-maps looked
like and how the post-cropping images looked like as well!
</p>
<p>
<p id="gdcalert3" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html
alert: inline image link here (to images_timeline/image3.png). Store image on your image
server and adjust path/filename/extension if necessary. </span><br>(<a
href="#">Back to top</a>)(<a href="#gdcalert4">Next alert</a>)<br><span
style="color: red; font-weight: bold">>>>>> </span></p>
<img src="images_timeline/image3.png" width="" alt="alt_text" title="image_tooltip">
</p>
<p>
 Then, we were contacted by a ‚Äòmeta-team‚Äô ethicist who presumably worked for
Twitter on October 6th for a discussion on the analysis. Well, if I recall
correctly, two of them did show up for the meeting during which we basically
conveyed our ‚ÄòAin‚Äôt no right way to do the wrong thing‚Äô belief (that we still
hold) and the Saliency cropping‚ÜíIntellectual quicksand‚Üí Dustbin-of-history
spiel. Besides that and the pleasantries exchanged, nothing much came of it.
</p>
<p>
<p id="gdcalert4" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html
alert: inline image link here (to images_timeline/image4.png). Store image on your image
server and adjust path/filename/extension if necessary. </span><br>(<a
href="#">Back to top</a>)(<a href="#gdcalert5">Next alert</a>)<br><span
style="color: red; font-weight: bold">>>>>> </span></p>
<img src="images_timeline/image4.png" width="" alt="alt_text" title="image_tooltip">
</p>
<p>
Then, through Dec-Mar, we experimented with different datasets and saw how
wildly the survival ratios fluctuated when we changed one seemingly
insignificant dataset curation parameter (in the context of 3x1 image grids).
</p>
<p>
In this period, we also extensively studied the literature surrounding saliency
estimation and saliency cropping (helped in no small measure by the truly
brilliant <a href="https://github.com/alexanderkroner">Alexander Kroner</a>, the
lead author of the SoTA <a
href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hWKtP0sAAAAJ&alert_preview_top_rm=2&citation_for_view=hWKtP0sAAAAJ:u5HHmVD_uO8C">Contextual
encoder-decoder network for visual saliency prediction</a> paper) and wrote and
rewrote the first draft of our paper:
</p>
<p>
 ‚ÄúIf saliency cropping is the answer, what is the question?‚Äù[<a
href="https://drive.google.com/file/d/1pt1MY29mrnNrW_pQ__FPcWFjJdbxOZmq/view?usp=sharing">paper</a>]
[<a
href="https://drive.google.com/file/d/1dNMrd4qoobiN1hRGdzXGwXMqN9y1LMQu/view?usp=sharing">poster</a>]
</p>
<p>
On April 2, 2021, we finally submitted this to the <a
href="https://sites.google.com/view/beyond-fairness-cv/accepted-papers?authuser=0">BeyondFairCV
CVPR workshop</a>.
</p>
<p>
<p id="gdcalert5" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html
alert: inline image link here (to images_timeline/image5.png). Store image on your image
server and adjust path/filename/extension if necessary. </span><br>(<a
href="#">Back to top</a>)(<a href="#gdcalert6">Next alert</a>)<br><span
style="color: red; font-weight: bold">>>>>> </span></p>
<img src="images_timeline/image5.png" width="" alt="alt_text" title="image_tooltip">
</p>
<p>
On Sun, May 23, 10:55 AM , we were notified by the workshop organizers that our
paper had been accepted!<br>Here, we are sharing the (blind) reviews that
we received:
</p>
<p>
<em>SUBMISSION: 21</em>
</p>
<p>
<em>TITLE: If saliency cropping is the answer, what is the question?</em>
</p>
<p>
<em>----------------------- REVIEW 1 ---------------------</em>
</p>
<p>
<em>SUBMISSION: 21</em>
</p>
<p>
<em>TITLE: If saliency cropping is the answer, what is the question?</em>
</p>
<p>
<em>AUTHORS: Vinay Prabhu and Abeba Birhane</em>
</p>
<p>
<em>----------- Overall evaluation -----------</em>
</p>
<p>
<em>SCORE: 2 (accept)</em>
</p>
<p>
<em>----- TEXT:</em>
</p>
<p>
<em>This is a very interesting study on an important problem online: image
cropping. With Twitter getting in trouble recently because of the biases
experienced using a saliency-based cropping mechanism. The authors first discuss
the issues with how hazy the definition of saliency is in current CV literature
and the contradictory expectations of such systems. Then they analyse the
Twitter case specifically and find that perceived miscroppings were on account
of either Separated saliency barycenters or saliency mismatch.</em>
</p>
<p>
<em>----------------------- REVIEW 2 ---------------------</em>
</p>
<p>
<em>SUBMISSION: 21</em>
</p>
<p>
<em>TITLE: If saliency cropping is the answer, what is the question?</em>
</p>
<p>
<em>AUTHORS: Vinay Prabhu and Abeba Birhane</em>
</p>
<p>
<em>----------- Overall evaluation -----------</em>
</p>
<p>
<em>SCORE: 3 (strong accept)</em>
</p>
<p>
<em>----- TEXT:</em>
</p>
<p>
<em>Summary:</em>
</p>
<p>
<em>This paper expands on recently noticed biases with the Twitter saliency
cropping algorithm, showing ethnicity/race and gender biases, in addition
pointing our the presence of the male gaze in saliency algorithms. It then
complicates the notion of saliency, and provides an overview of different
definitions and schools of development of saliency. The paper ends by
challenging the nessecity of saliency cropping in appplications like
Twitter.</em>
</p>
<p>
<em>Pros:</em>
</p>
<p>
<em>This paper is a thorough and contextual exmination of biases of saliency
algorithms, and poses thoughful challenges to the notion of saliency
itself.</em>
</p>
<p>
<em>-Includes extensive examples and code</em>
</p>
<p>
<em>-Detailed, multidiscislenary background and discussion</em>
</p>
<p>
<em>-Evaluation of Twitter saliency algorithm on hundreds of cropping tests</em>
</p>
<p>
<em>Cons:</em>
</p>
<p>
<em>Other:</em>
</p>
<p>
<em>Have the authors considered which images_timeline/videos are labelled as potentially
sensative content on Twitter? In my own experince I've noticed that media from
or depecting Black people are disproportionately labelled as such.</em>
</p>
<p>
On June 25, 2021, we presented the paper/poster via their discord channel and
received plentiful feedback during the presentation.
</p>
<p>
Some of the crucial feedback we received informed our next round of
experimentation ( Male-gaze-like cropping investigation using the source images
from Twitter and the CFD 3x1 grid images that were adherent to the original
ratios of the original <a
href="https://www.theguardian.com/technology/2020/sep/21/twitter-apologises-for-racist-image-cropping-algorithm">viral
tweet</a> )
</p>
<p>
<p id="gdcalert6" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html
alert: inline image link here (to images_timeline/image6.png). Store image on your image
server and adjust path/filename/extension if necessary. </span><br>(<a
href="#">Back to top</a>)(<a href="#gdcalert7">Next alert</a>)<br><span
style="color: red; font-weight: bold">>>>>> </span></p>
<img src="images_timeline/image6.png" width="" alt="alt_text" title="image_tooltip">
</p>
<p>
We collected all this feedback and targeted the second round of the <a
href="https://wacv2022.thecvf.com/node/75">WACV-2022 conference</a> where we had
published our <a href="https://arxiv.org/abs/2006.16923">Large image datasets: A
pyrrhic win for computer vision?</a> paper the previous year.
</p>
<p>
On Oct 4, 2021, we received the acceptance notification at WACV-2022  üôå. Looks
like the meta-reviewer bailed us out!<br>[Here are the <a
href="https://github.com/vinayprabhu/Saliency_Image_Cropping/blob/main/paper/Reviews_wacv_2022.pdf">reviews</a>
for the reader‚Äôs reference]
</p>
<p>
<strong>Where to now?</strong>
</p>
<p>
This is work in progress where we are still working on problems such as the fate
of saliency cropping models initialized with a pre-trained vision model trained
on
</p>
<ol>
<li>The recently released face-blurred version of the ImageNet dataset.
<li>The <a href="https://arxiv.org/abs/2109.13228">PASS dataset</a>
<li><a href="https://arxiv.org/abs/2110.01963">Multimodal datasets</a> such as
LAION-400M
<p>
        Feel free to get in touch with us if you are interested in
collaborating!
</p>
</li>
</ol>
